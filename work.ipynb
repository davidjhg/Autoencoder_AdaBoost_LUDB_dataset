{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 및 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import wfdb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'record_name': '19',\n",
       " 'n_sig': 12,\n",
       " 'fs': 500,\n",
       " 'counter_freq': None,\n",
       " 'base_counter': None,\n",
       " 'sig_len': 5000,\n",
       " 'base_time': None,\n",
       " 'base_date': None,\n",
       " 'comments': ['<age>: 66',\n",
       "  '<sex>: M',\n",
       "  '<diagnoses>:',\n",
       "  'Electric axis of the heart: left axis deviation.',\n",
       "  'Incomplete left bundle branch block.',\n",
       "  'Left atrial hypertrophy.',\n",
       "  'Left ventricular hypertrophy.',\n",
       "  'Non-specific repolarization abnormalities: posterior wall.'],\n",
       " 'sig_name': ['i',\n",
       "  'ii',\n",
       "  'iii',\n",
       "  'avr',\n",
       "  'avl',\n",
       "  'avf',\n",
       "  'v1',\n",
       "  'v2',\n",
       "  'v3',\n",
       "  'v4',\n",
       "  'v5',\n",
       "  'v6'],\n",
       " 'p_signal': array([[ -56.98863424, -167.0007552 , -110.00515288, ...,  -61.99617987,\n",
       "         -255.00685349,   -9.99082475],\n",
       "        [ -67.99204983, -149.99487213,  -81.99741404, ...,  -67.01414743,\n",
       "         -255.00685349,  -20.0156319 ],\n",
       "        [ -44.9994405 , -152.0087267 , -107.00043915, ...,  -52.01420139,\n",
       "         -242.01378313,    0.        ],\n",
       "        ...,\n",
       "        [ 187.00477969,  179.99757592,   -7.00193471, ...,  140.01748195,\n",
       "          204.98591228,  149.99830088],\n",
       "        [ 170.00676723,  197.00345898,   27.0016356 , ...,  181.99574822,\n",
       "          204.98591228,  124.9872566 ],\n",
       "        [  61.99745296,   82.99691395,   21.00580413, ...,   88.97450009,\n",
       "           88.00068535,   72.00869949]]),\n",
       " 'd_signal': None,\n",
       " 'e_p_signal': None,\n",
       " 'e_d_signal': None,\n",
       " 'file_name': ['19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat',\n",
       "  '19.dat'],\n",
       " 'fmt': ['16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16',\n",
       "  '16'],\n",
       " 'samps_per_frame': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'skew': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'byte_offset': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'adc_gain': [37.5338,\n",
       "  53.6285,\n",
       "  73.5511,\n",
       "  46.4451,\n",
       "  50.6445,\n",
       "  106.5593,\n",
       "  27.0578,\n",
       "  37.7935,\n",
       "  23.8392,\n",
       "  18.5334,\n",
       "  21.0112,\n",
       "  29.427],\n",
       " 'baseline': [-27062,\n",
       "  -22041,\n",
       "  17542,\n",
       "  26450,\n",
       "  -24056,\n",
       "  -15291,\n",
       "  18968,\n",
       "  11338,\n",
       "  -3874,\n",
       "  -14382,\n",
       "  -22808,\n",
       "  -27706],\n",
       " 'units': ['mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV',\n",
       "  'mV'],\n",
       " 'adc_res': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'adc_zero': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'init_value': [-29201,\n",
       "  -30997,\n",
       "  9451,\n",
       "  31652,\n",
       "  -22739,\n",
       "  -30103,\n",
       "  29439,\n",
       "  16440,\n",
       "  -2396,\n",
       "  -15531,\n",
       "  -28166,\n",
       "  -28000],\n",
       " 'checksum': [6931,\n",
       "  -17677,\n",
       "  -749,\n",
       "  7598,\n",
       "  -31995,\n",
       "  28109,\n",
       "  28028,\n",
       "  28816,\n",
       "  -26718,\n",
       "  20389,\n",
       "  13007,\n",
       "  -17596],\n",
       " 'block_size': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wfdb\n",
    "record = wfdb.rdrecord('1.0.0/19') \n",
    "wfdb.plot_wfdb(record=record, title='record') \n",
    "display(record.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 \n",
    "datas = []\n",
    "for i in range(1,201):\n",
    "    record = wfdb.rdrecord('1.0.0/'+str(i))\n",
    "    datas.append(record)\n",
    "\n",
    "datas[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 레이블링 수행 \n",
    "Non-specific repolarization abnormalities 유무를 통해서 특정 환자의 위험성 여부를 예측하고 병원에서 검진을 권고해주기 위한 분류기를 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "target = 'Non-specific repolarization abnormalities'\n",
    "labels = []\n",
    "\n",
    "for data in datas:\n",
    "    s = ''.join(data.comments)\n",
    "    if target in s:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련, 검증, 테스트 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 160, 20, 20, 20, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainDatas,testDatas, trainLabels,testLabels = train_test_split(datas,labels,test_size=0.2,random_state = 42,stratify=labels)\n",
    "validDatas,testDatas, validLabels, testLabels = train_test_split(testDatas,testLabels,test_size=0.5,random_state = 42,stratify=testLabels)\n",
    "len(trainDatas),len(trainLabels),len(validDatas),len(validLabels),len(testDatas),len(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 136, 1: 44}), Counter({0: 15, 1: 5}))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(trainLabels+validLabels),Counter(testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p signal 을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 12, 5000), (20, 12, 5000), (20, 12, 5000))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataToMat(datas):\n",
    "    mat = np.zeros(shape=(len(datas),12,5000))\n",
    "    for i in range(len(datas)):\n",
    "        mat[i] = datas[i].p_signal.T\n",
    "    return mat\n",
    "trainMat = dataToMat(trainDatas)\n",
    "validMat = dataToMat(validDatas)\n",
    "testMat = dataToMat(testDatas)\n",
    "trainMat.shape,validMat.shape,testMat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-dimensional Convolutional AutoEncoder 구현\n",
    "Encoder는 3개의 1d convolution 3개의 ReLU unit이 Decoder에는 3개의 de-convolution과 4개의 ReLU, 1개의 convolution으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class AutoEncoderV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoderV1,self).__init__()\n",
    "        self.n_chan = [16,32,64]\n",
    "        n_filter = 9\n",
    "        n_stride = 5\n",
    "        self.Encoder = nn.Sequential(*[\n",
    "            nn.Conv1d(12,self.n_chan[0],n_filter,n_stride),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.n_chan[0],self.n_chan[1],n_filter,n_stride),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.n_chan[1],self.n_chan[2],n_filter,n_stride),\n",
    "            nn.ReLU(True)\n",
    "        ])\n",
    "        \n",
    "        self.Decoder = nn.Sequential(*[\n",
    "            nn.ConvTranspose1d(self.n_chan[2],self.n_chan[1],n_filter,n_stride),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(self.n_chan[1],self.n_chan[0],n_filter,n_stride),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(self.n_chan[0],12,n_filter,n_stride,output_padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(12,12,3,1,1),\n",
    "            nn.ReLU(True)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Encoder(x)\n",
    "        x = self.Decoder(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.Encoder(x).view((batch_size,-1))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 클래스 구성 및 데이터 로더 구성\n",
    "MSE 손실함수를 오토인코더의 재구성 오차 함수로 선택\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 1\n"
     ]
    }
   ],
   "source": [
    "class MatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,mat):\n",
    "        self.mat = mat\n",
    "    def __len__(self):\n",
    "        return len(self.mat)\n",
    "    def __getitem__(self,i):\n",
    "        return self.mat[i]\n",
    "trainDataset = MatDataset(trainMat)\n",
    "validDataset = MatDataset(validMat)\n",
    "testDataset = MatDataset(testMat)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset,batch_size=20,num_workers=4)\n",
    "validLoader = torch.utils.data.DataLoader(validDataset,batch_size=20,num_workers=1)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset,batch_size=20,num_workers=1)\n",
    "\n",
    "print(len(trainLoader),len(validLoader),len(testLoader))\n",
    "\n",
    "\n",
    "def trainModel(model,trainLoader,optimizer,num_print=20):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    training_correct = 0\n",
    "    total = 0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    for idx,data in enumerate(trainLoader):\n",
    "        data = data.float().cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output,data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    training_loss = training_loss/len(trainLoader)\n",
    "    \n",
    "    return training_loss\n",
    "\n",
    "\n",
    "def testModel(model,testLoader,return_prediction=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    total = 0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for idx,data in enumerate(testLoader):\n",
    "            data = data.float().cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output,data)\n",
    "            test_loss+=loss.item()\n",
    "            \n",
    "    test_loss = test_loss/len(testLoader)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "import copy\n",
    "def loop(model,trainLoader,validLoader,optimizer,epochs = 20000):\n",
    "    best_loss = None\n",
    "    best_model = None\n",
    "    history = {}\n",
    "    history['loss'] = []\n",
    "    history['valid_loss'] = []\n",
    "    pat_max = 100\n",
    "    pat = 0\n",
    "    for i in range(epochs):\n",
    "        loss = trainModel(model,trainLoader,optimizer)\n",
    "        \n",
    "        valid_loss = testModel(model, validLoader,optimizer)\n",
    "        print('epoch: %d'%(i),end=' ')\n",
    "        print('train loss: ',loss,end=', '); print('valid loss: ',valid_loss)\n",
    "        if best_loss is None:\n",
    "            best_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('patience is reset, best model is updated')\n",
    "            pat = 0\n",
    "            \n",
    "        else:\n",
    "            pat += 1\n",
    "            if pat > pat_max: break\n",
    "            \n",
    "            \n",
    "        history['loss'].append(loss)\n",
    "        history['valid_loss'].append(valid_loss)\n",
    "    model.load_state_dict(best_model)\n",
    "    return model,best_loss,history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오토인코더 학습 및 조기 종료\n",
    "오토인코더는 훈련 데이터에 대해서 학습 후 검증 데이터로 재구성 오차를 계산\n",
    "재구성 오차가 100 에폭 동안 감소하지 않으면 재구성 오차 함수가 가장 적었던 최적의 모델 파라메터로 모델을 설정한 후에 훈련 종료\n",
    "모델은 1258에폭을 학습한 후에 오차가 줄어들지 않아 조기 종료 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss:  61357.14013671875, valid loss:  61925.046875\n",
      "epoch: 1 train loss:  61236.10595703125, valid loss:  61459.38671875\n",
      "patience is reset, best model is updated\n",
      "epoch: 2 train loss:  60359.20849609375, valid loss:  60311.51171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 3 train loss:  58738.1357421875, valid loss:  58426.34765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 4 train loss:  56684.7197265625, valid loss:  56722.34375\n",
      "patience is reset, best model is updated\n",
      "epoch: 5 train loss:  54959.503173828125, valid loss:  55822.30859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 6 train loss:  53549.35888671875, valid loss:  55479.76171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 7 train loss:  52399.73583984375, valid loss:  56036.45703125\n",
      "epoch: 8 train loss:  52126.55419921875, valid loss:  55693.5703125\n",
      "epoch: 9 train loss:  50871.518310546875, valid loss:  55188.62890625\n",
      "patience is reset, best model is updated\n",
      "epoch: 10 train loss:  49126.15576171875, valid loss:  54719.71484375\n",
      "patience is reset, best model is updated\n",
      "epoch: 11 train loss:  49297.278564453125, valid loss:  53376.27734375\n",
      "patience is reset, best model is updated\n",
      "epoch: 12 train loss:  48465.340576171875, valid loss:  53198.390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 13 train loss:  49574.254638671875, valid loss:  51643.30859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 14 train loss:  47108.103759765625, valid loss:  50975.85546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 15 train loss:  45236.61767578125, valid loss:  49372.90625\n",
      "patience is reset, best model is updated\n",
      "epoch: 16 train loss:  44277.48193359375, valid loss:  48356.6640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 17 train loss:  43595.1982421875, valid loss:  46958.16796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 18 train loss:  42851.92431640625, valid loss:  45564.29296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 19 train loss:  42094.2724609375, valid loss:  44078.609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 20 train loss:  41200.28369140625, valid loss:  42796.26953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 21 train loss:  40414.275390625, valid loss:  41552.47265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 22 train loss:  39643.494140625, valid loss:  40788.5703125\n",
      "patience is reset, best model is updated\n",
      "epoch: 23 train loss:  39469.98291015625, valid loss:  40249.5\n",
      "patience is reset, best model is updated\n",
      "epoch: 24 train loss:  39006.2734375, valid loss:  39907.921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 25 train loss:  39582.467041015625, valid loss:  39403.92578125\n",
      "patience is reset, best model is updated\n",
      "epoch: 26 train loss:  37975.779296875, valid loss:  39084.65625\n",
      "patience is reset, best model is updated\n",
      "epoch: 27 train loss:  37663.591552734375, valid loss:  38453.75390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 28 train loss:  37398.10693359375, valid loss:  38244.609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 29 train loss:  37196.8876953125, valid loss:  37964.8359375\n",
      "patience is reset, best model is updated\n",
      "epoch: 30 train loss:  37021.498779296875, valid loss:  37856.9609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 31 train loss:  36882.305419921875, valid loss:  37676.1875\n",
      "patience is reset, best model is updated\n",
      "epoch: 32 train loss:  36803.935302734375, valid loss:  37601.5\n",
      "patience is reset, best model is updated\n",
      "epoch: 33 train loss:  36748.3095703125, valid loss:  37386.46875\n",
      "patience is reset, best model is updated\n",
      "epoch: 34 train loss:  37133.1611328125, valid loss:  37546.1484375\n",
      "epoch: 35 train loss:  37296.098388671875, valid loss:  37397.328125\n",
      "epoch: 36 train loss:  38913.83203125, valid loss:  37397.6953125\n",
      "epoch: 37 train loss:  36482.583740234375, valid loss:  37117.69140625\n",
      "patience is reset, best model is updated\n",
      "epoch: 38 train loss:  37287.9853515625, valid loss:  36720.40625\n",
      "patience is reset, best model is updated\n",
      "epoch: 39 train loss:  38317.510009765625, valid loss:  36680.1015625\n",
      "patience is reset, best model is updated\n",
      "epoch: 40 train loss:  36448.2421875, valid loss:  36622.4453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 41 train loss:  37069.56494140625, valid loss:  36224.40234375\n",
      "patience is reset, best model is updated\n",
      "epoch: 42 train loss:  37435.0849609375, valid loss:  36136.26171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 43 train loss:  36057.555419921875, valid loss:  35958.3125\n",
      "patience is reset, best model is updated\n",
      "epoch: 44 train loss:  36145.07666015625, valid loss:  35598.890625\n",
      "patience is reset, best model is updated\n",
      "epoch: 45 train loss:  35898.927001953125, valid loss:  35487.5390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 46 train loss:  35341.489990234375, valid loss:  35328.453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 47 train loss:  35341.822021484375, valid loss:  35112.5078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 48 train loss:  35184.89794921875, valid loss:  34996.91796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 49 train loss:  35009.769287109375, valid loss:  34885.22265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 50 train loss:  34932.94677734375, valid loss:  34758.27734375\n",
      "patience is reset, best model is updated\n",
      "epoch: 51 train loss:  34835.43603515625, valid loss:  34659.390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 52 train loss:  34738.274169921875, valid loss:  34550.55859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 53 train loss:  34654.628173828125, valid loss:  34453.9453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 54 train loss:  34573.63330078125, valid loss:  34366.0390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 55 train loss:  34494.497802734375, valid loss:  34274.91796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 56 train loss:  34413.535400390625, valid loss:  34194.0546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 57 train loss:  34334.625732421875, valid loss:  34115.81640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 58 train loss:  34257.83984375, valid loss:  34045.0234375\n",
      "patience is reset, best model is updated\n",
      "epoch: 59 train loss:  34184.46337890625, valid loss:  33968.5703125\n",
      "patience is reset, best model is updated\n",
      "epoch: 60 train loss:  34113.392822265625, valid loss:  33893.4765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 61 train loss:  34041.27197265625, valid loss:  33780.98828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 62 train loss:  33971.190673828125, valid loss:  33716.53515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 63 train loss:  33909.013427734375, valid loss:  33521.19921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 64 train loss:  33900.716064453125, valid loss:  33530.51171875\n",
      "epoch: 65 train loss:  34013.298095703125, valid loss:  33344.359375\n",
      "patience is reset, best model is updated\n",
      "epoch: 66 train loss:  34523.952392578125, valid loss:  33436.56640625\n",
      "epoch: 67 train loss:  33974.27734375, valid loss:  33371.046875\n",
      "epoch: 68 train loss:  33824.799560546875, valid loss:  33359.375\n",
      "epoch: 69 train loss:  33699.48486328125, valid loss:  33260.64453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 70 train loss:  33541.98828125, valid loss:  33118.1953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 71 train loss:  33487.96923828125, valid loss:  33096.5625\n",
      "patience is reset, best model is updated\n",
      "epoch: 72 train loss:  33392.426025390625, valid loss:  32945.9296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 73 train loss:  33388.040771484375, valid loss:  32941.12890625\n",
      "patience is reset, best model is updated\n",
      "epoch: 74 train loss:  33366.455810546875, valid loss:  32847.4140625\n",
      "patience is reset, best model is updated\n",
      "epoch: 75 train loss:  33342.6435546875, valid loss:  32873.390625\n",
      "epoch: 76 train loss:  33337.904296875, valid loss:  32781.3203125\n",
      "patience is reset, best model is updated\n",
      "epoch: 77 train loss:  33274.526611328125, valid loss:  32809.234375\n",
      "epoch: 78 train loss:  33254.2216796875, valid loss:  32724.9921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 79 train loss:  33170.837158203125, valid loss:  32785.91015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80 train loss:  33139.9931640625, valid loss:  32690.947265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 81 train loss:  33072.738037109375, valid loss:  32711.130859375\n",
      "epoch: 82 train loss:  33054.818359375, valid loss:  32601.326171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 83 train loss:  32973.95556640625, valid loss:  32545.505859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 84 train loss:  32968.904296875, valid loss:  32378.974609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 85 train loss:  32834.718994140625, valid loss:  32305.431640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 86 train loss:  32758.34375, valid loss:  32173.974609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 87 train loss:  32569.701171875, valid loss:  32120.98828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 88 train loss:  32448.698486328125, valid loss:  32061.611328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 89 train loss:  32303.667724609375, valid loss:  31980.400390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 90 train loss:  32207.9775390625, valid loss:  31938.169921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 91 train loss:  32110.288818359375, valid loss:  31845.24609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 92 train loss:  32073.81591796875, valid loss:  31797.453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 93 train loss:  32022.816650390625, valid loss:  31703.078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 94 train loss:  32063.826416015625, valid loss:  31692.59765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 95 train loss:  32003.463134765625, valid loss:  31617.4296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 96 train loss:  32133.25146484375, valid loss:  31657.158203125\n",
      "epoch: 97 train loss:  31960.850341796875, valid loss:  31553.140625\n",
      "patience is reset, best model is updated\n",
      "epoch: 98 train loss:  32046.699462890625, valid loss:  31581.169921875\n",
      "epoch: 99 train loss:  31837.61376953125, valid loss:  31468.326171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 100 train loss:  31891.941162109375, valid loss:  31483.662109375\n",
      "epoch: 101 train loss:  31716.909912109375, valid loss:  31394.109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 102 train loss:  31741.485595703125, valid loss:  31375.01953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 103 train loss:  31616.74609375, valid loss:  31295.00390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 104 train loss:  31680.302490234375, valid loss:  31286.791015625\n",
      "patience is reset, best model is updated\n",
      "epoch: 105 train loss:  31545.779541015625, valid loss:  31206.76171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 106 train loss:  31625.048095703125, valid loss:  31200.42578125\n",
      "patience is reset, best model is updated\n",
      "epoch: 107 train loss:  31473.52978515625, valid loss:  31123.63671875\n",
      "patience is reset, best model is updated\n",
      "epoch: 108 train loss:  31549.41162109375, valid loss:  31126.333984375\n",
      "epoch: 109 train loss:  31392.158203125, valid loss:  31057.701171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 110 train loss:  31457.732177734375, valid loss:  31077.921875\n",
      "epoch: 111 train loss:  31314.21484375, valid loss:  31026.18359375\n",
      "patience is reset, best model is updated\n",
      "epoch: 112 train loss:  31391.686279296875, valid loss:  31042.044921875\n",
      "epoch: 113 train loss:  31246.750732421875, valid loss:  30999.056640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 114 train loss:  31327.161376953125, valid loss:  31018.126953125\n",
      "epoch: 115 train loss:  31167.297119140625, valid loss:  30946.0\n",
      "patience is reset, best model is updated\n",
      "epoch: 116 train loss:  31231.114501953125, valid loss:  30944.64453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 117 train loss:  31085.77392578125, valid loss:  30870.33984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 118 train loss:  31141.9267578125, valid loss:  30860.705078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 119 train loss:  31012.462158203125, valid loss:  30797.140625\n",
      "patience is reset, best model is updated\n",
      "epoch: 120 train loss:  31074.948974609375, valid loss:  30763.697265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 121 train loss:  30958.077880859375, valid loss:  30712.375\n",
      "patience is reset, best model is updated\n",
      "epoch: 122 train loss:  31036.612060546875, valid loss:  30687.306640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 123 train loss:  30900.19140625, valid loss:  30656.357421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 124 train loss:  30963.680419921875, valid loss:  30605.9296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 125 train loss:  30838.58349609375, valid loss:  30599.208984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 126 train loss:  30905.943359375, valid loss:  30559.453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 127 train loss:  30784.159423828125, valid loss:  30560.955078125\n",
      "epoch: 128 train loss:  30855.413330078125, valid loss:  30525.234375\n",
      "patience is reset, best model is updated\n",
      "epoch: 129 train loss:  30734.244384765625, valid loss:  30521.849609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 130 train loss:  30803.631591796875, valid loss:  30496.236328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 131 train loss:  30687.836181640625, valid loss:  30491.01953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 132 train loss:  30767.255615234375, valid loss:  30486.701171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 133 train loss:  30656.69384765625, valid loss:  30471.798828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 134 train loss:  30740.71435546875, valid loss:  30473.458984375\n",
      "epoch: 135 train loss:  30609.76953125, valid loss:  30455.828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 136 train loss:  30677.80908203125, valid loss:  30469.4140625\n",
      "epoch: 137 train loss:  30570.840087890625, valid loss:  30446.1953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 138 train loss:  30643.0185546875, valid loss:  30475.28125\n",
      "epoch: 139 train loss:  30537.182861328125, valid loss:  30427.123046875\n",
      "patience is reset, best model is updated\n",
      "epoch: 140 train loss:  30610.7412109375, valid loss:  30477.017578125\n",
      "epoch: 141 train loss:  30524.82275390625, valid loss:  30421.1796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 142 train loss:  30610.84375, valid loss:  30454.2890625\n",
      "epoch: 143 train loss:  30514.03759765625, valid loss:  30374.81640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 144 train loss:  30595.84619140625, valid loss:  30382.884765625\n",
      "epoch: 145 train loss:  30498.11279296875, valid loss:  30297.265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 146 train loss:  30556.921630859375, valid loss:  30297.640625\n",
      "epoch: 147 train loss:  30457.9248046875, valid loss:  30309.642578125\n",
      "epoch: 148 train loss:  30507.7822265625, valid loss:  30413.39453125\n",
      "epoch: 149 train loss:  30422.75390625, valid loss:  30584.59375\n",
      "epoch: 150 train loss:  30477.2841796875, valid loss:  30632.296875\n",
      "epoch: 151 train loss:  30403.438232421875, valid loss:  30586.53125\n",
      "epoch: 152 train loss:  30457.666259765625, valid loss:  30217.205078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 153 train loss:  30399.282958984375, valid loss:  30254.720703125\n",
      "epoch: 154 train loss:  30423.530517578125, valid loss:  30096.357421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 155 train loss:  30342.352294921875, valid loss:  30260.71484375\n",
      "epoch: 156 train loss:  30404.12451171875, valid loss:  30044.689453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 157 train loss:  30317.22802734375, valid loss:  30094.099609375\n",
      "epoch: 158 train loss:  30380.541259765625, valid loss:  29983.859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 159 train loss:  30234.822998046875, valid loss:  30193.669921875\n",
      "epoch: 160 train loss:  30274.85791015625, valid loss:  30222.234375\n",
      "epoch: 161 train loss:  30203.121337890625, valid loss:  30369.51953125\n",
      "epoch: 162 train loss:  30240.088134765625, valid loss:  30280.501953125\n",
      "epoch: 163 train loss:  30201.428955078125, valid loss:  30471.11328125\n",
      "epoch: 164 train loss:  30224.2802734375, valid loss:  30381.478515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 165 train loss:  30176.5, valid loss:  30438.2421875\n",
      "epoch: 166 train loss:  30226.720703125, valid loss:  30164.603515625\n",
      "epoch: 167 train loss:  30152.63134765625, valid loss:  30181.92578125\n",
      "epoch: 168 train loss:  30198.00732421875, valid loss:  29967.0390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 169 train loss:  30065.000732421875, valid loss:  30113.072265625\n",
      "epoch: 170 train loss:  30085.610595703125, valid loss:  29996.66015625\n",
      "epoch: 171 train loss:  29967.575439453125, valid loss:  30069.07421875\n",
      "epoch: 172 train loss:  29985.677001953125, valid loss:  29906.265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 173 train loss:  29897.95654296875, valid loss:  29956.359375\n",
      "epoch: 174 train loss:  29918.195556640625, valid loss:  29817.220703125\n",
      "patience is reset, best model is updated\n",
      "epoch: 175 train loss:  29835.605712890625, valid loss:  29895.943359375\n",
      "epoch: 176 train loss:  29867.0849609375, valid loss:  29823.32421875\n",
      "epoch: 177 train loss:  29785.604736328125, valid loss:  29894.296875\n",
      "epoch: 178 train loss:  29823.5625, valid loss:  29859.8671875\n",
      "epoch: 179 train loss:  29747.714111328125, valid loss:  29913.892578125\n",
      "epoch: 180 train loss:  29790.637939453125, valid loss:  29883.111328125\n",
      "epoch: 181 train loss:  29710.953857421875, valid loss:  29909.845703125\n",
      "epoch: 182 train loss:  29750.883056640625, valid loss:  29884.462890625\n",
      "epoch: 183 train loss:  29675.143310546875, valid loss:  29897.775390625\n",
      "epoch: 184 train loss:  29724.06787109375, valid loss:  29859.03125\n",
      "epoch: 185 train loss:  29639.610107421875, valid loss:  29856.64453125\n",
      "epoch: 186 train loss:  29680.222900390625, valid loss:  29813.0703125\n",
      "patience is reset, best model is updated\n",
      "epoch: 187 train loss:  29595.748779296875, valid loss:  29826.494140625\n",
      "epoch: 188 train loss:  29644.07568359375, valid loss:  29758.302734375\n",
      "patience is reset, best model is updated\n",
      "epoch: 189 train loss:  29568.640625, valid loss:  29792.396484375\n",
      "epoch: 190 train loss:  29627.548095703125, valid loss:  29729.35546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 191 train loss:  29523.242431640625, valid loss:  29767.255859375\n",
      "epoch: 192 train loss:  29563.607666015625, valid loss:  29690.744140625\n",
      "patience is reset, best model is updated\n",
      "epoch: 193 train loss:  29475.872802734375, valid loss:  29738.55078125\n",
      "epoch: 194 train loss:  29514.768310546875, valid loss:  29669.287109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 195 train loss:  29435.34033203125, valid loss:  29736.93359375\n",
      "epoch: 196 train loss:  29482.427490234375, valid loss:  29645.50390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 197 train loss:  29403.53662109375, valid loss:  29721.3125\n",
      "epoch: 198 train loss:  29451.744140625, valid loss:  29632.208984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 199 train loss:  29360.91650390625, valid loss:  29705.771484375\n",
      "epoch: 200 train loss:  29400.138671875, valid loss:  29606.0390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 201 train loss:  29319.095947265625, valid loss:  29681.736328125\n",
      "epoch: 202 train loss:  29363.37060546875, valid loss:  29573.078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 203 train loss:  29285.995361328125, valid loss:  29660.05078125\n",
      "epoch: 204 train loss:  29338.819580078125, valid loss:  29550.888671875\n",
      "patience is reset, best model is updated\n",
      "epoch: 205 train loss:  29238.072021484375, valid loss:  29622.736328125\n",
      "epoch: 206 train loss:  29274.15576171875, valid loss:  29498.251953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 207 train loss:  29188.627685546875, valid loss:  29573.73828125\n",
      "epoch: 208 train loss:  29226.498779296875, valid loss:  29464.458984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 209 train loss:  29136.573486328125, valid loss:  29527.517578125\n",
      "epoch: 210 train loss:  29179.094970703125, valid loss:  29392.09765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 211 train loss:  29106.4736328125, valid loss:  29500.087890625\n",
      "epoch: 212 train loss:  29155.13330078125, valid loss:  29363.52734375\n",
      "patience is reset, best model is updated\n",
      "epoch: 213 train loss:  29044.697021484375, valid loss:  29432.755859375\n",
      "epoch: 214 train loss:  29070.296875, valid loss:  29304.4765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 215 train loss:  28982.4287109375, valid loss:  29390.564453125\n",
      "epoch: 216 train loss:  29005.734130859375, valid loss:  29241.859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 217 train loss:  28936.773193359375, valid loss:  29347.826171875\n",
      "epoch: 218 train loss:  28973.572021484375, valid loss:  29195.984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 219 train loss:  28905.36376953125, valid loss:  29331.17578125\n",
      "epoch: 220 train loss:  28933.240234375, valid loss:  29159.177734375\n",
      "patience is reset, best model is updated\n",
      "epoch: 221 train loss:  28865.717041015625, valid loss:  29293.404296875\n",
      "epoch: 222 train loss:  28898.31884765625, valid loss:  29107.0078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 223 train loss:  28851.27392578125, valid loss:  29283.64453125\n",
      "epoch: 224 train loss:  28899.84912109375, valid loss:  29065.134765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 225 train loss:  28829.1318359375, valid loss:  29218.935546875\n",
      "epoch: 226 train loss:  28858.859375, valid loss:  29008.412109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 227 train loss:  28789.0322265625, valid loss:  29143.18359375\n",
      "epoch: 228 train loss:  28822.598876953125, valid loss:  28928.716796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 229 train loss:  28762.3974609375, valid loss:  29025.935546875\n",
      "epoch: 230 train loss:  28802.338623046875, valid loss:  28819.736328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 231 train loss:  28745.3125, valid loss:  28907.39453125\n",
      "epoch: 232 train loss:  28800.1123046875, valid loss:  28765.982421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 233 train loss:  28726.074951171875, valid loss:  28821.421875\n",
      "epoch: 234 train loss:  28784.6689453125, valid loss:  28807.943359375\n",
      "epoch: 235 train loss:  28705.084228515625, valid loss:  28864.423828125\n",
      "epoch: 236 train loss:  28786.367431640625, valid loss:  29032.19140625\n",
      "epoch: 237 train loss:  28714.871826171875, valid loss:  29062.390625\n",
      "epoch: 238 train loss:  28802.33544921875, valid loss:  29205.203125\n",
      "epoch: 239 train loss:  28722.312744140625, valid loss:  28992.35546875\n",
      "epoch: 240 train loss:  28756.13916015625, valid loss:  28813.701171875\n",
      "epoch: 241 train loss:  28672.62841796875, valid loss:  28723.685546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 242 train loss:  28639.944091796875, valid loss:  28702.201171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 243 train loss:  28567.566162109375, valid loss:  28863.771484375\n",
      "epoch: 244 train loss:  28541.13720703125, valid loss:  28811.802734375\n",
      "epoch: 245 train loss:  28514.8154296875, valid loss:  28921.060546875\n",
      "epoch: 246 train loss:  28531.423095703125, valid loss:  28728.623046875\n",
      "epoch: 247 train loss:  28503.199462890625, valid loss:  28762.8046875\n",
      "epoch: 248 train loss:  28529.861083984375, valid loss:  28587.853515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 249 train loss:  28467.844970703125, valid loss:  28630.599609375\n",
      "epoch: 250 train loss:  28507.320068359375, valid loss:  28612.642578125\n",
      "epoch: 251 train loss:  28441.426025390625, valid loss:  28659.58984375\n",
      "epoch: 252 train loss:  28494.235107421875, valid loss:  28702.26171875\n",
      "epoch: 253 train loss:  28421.3525390625, valid loss:  28674.287109375\n",
      "epoch: 254 train loss:  28461.210205078125, valid loss:  28662.1875\n",
      "epoch: 255 train loss:  28399.99169921875, valid loss:  28639.560546875\n",
      "epoch: 256 train loss:  28437.409423828125, valid loss:  28577.484375\n",
      "patience is reset, best model is updated\n",
      "epoch: 257 train loss:  28368.169189453125, valid loss:  28569.947265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 258 train loss:  28381.1474609375, valid loss:  28499.169921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 259 train loss:  28327.635986328125, valid loss:  28564.8828125\n",
      "epoch: 260 train loss:  28352.374267578125, valid loss:  28477.0234375\n",
      "patience is reset, best model is updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 261 train loss:  28319.958740234375, valid loss:  28572.484375\n",
      "epoch: 262 train loss:  28349.76513671875, valid loss:  28492.830078125\n",
      "epoch: 263 train loss:  28301.201416015625, valid loss:  28603.099609375\n",
      "epoch: 264 train loss:  28329.1533203125, valid loss:  28476.5546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 265 train loss:  28287.904541015625, valid loss:  28608.005859375\n",
      "epoch: 266 train loss:  28312.109375, valid loss:  28476.240234375\n",
      "patience is reset, best model is updated\n",
      "epoch: 267 train loss:  28267.388427734375, valid loss:  28599.837890625\n",
      "epoch: 268 train loss:  28298.13525390625, valid loss:  28431.421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 269 train loss:  28260.14794921875, valid loss:  28574.501953125\n",
      "epoch: 270 train loss:  28292.3173828125, valid loss:  28425.3515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 271 train loss:  28247.139404296875, valid loss:  28549.921875\n",
      "epoch: 272 train loss:  28287.324951171875, valid loss:  28383.21875\n",
      "patience is reset, best model is updated\n",
      "epoch: 273 train loss:  28244.121337890625, valid loss:  28534.60546875\n",
      "epoch: 274 train loss:  28283.312255859375, valid loss:  28410.3359375\n",
      "epoch: 275 train loss:  28219.54833984375, valid loss:  28561.66796875\n",
      "epoch: 276 train loss:  28249.877197265625, valid loss:  28445.341796875\n",
      "epoch: 277 train loss:  28195.345458984375, valid loss:  28569.927734375\n",
      "epoch: 278 train loss:  28228.688720703125, valid loss:  28473.599609375\n",
      "epoch: 279 train loss:  28180.39111328125, valid loss:  28517.986328125\n",
      "epoch: 280 train loss:  28209.277587890625, valid loss:  28396.455078125\n",
      "epoch: 281 train loss:  28160.171142578125, valid loss:  28367.451171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 282 train loss:  28173.494384765625, valid loss:  28315.763671875\n",
      "patience is reset, best model is updated\n",
      "epoch: 283 train loss:  28123.512939453125, valid loss:  28327.5234375\n",
      "epoch: 284 train loss:  28140.8671875, valid loss:  28399.0390625\n",
      "epoch: 285 train loss:  28112.933349609375, valid loss:  28415.34765625\n",
      "epoch: 286 train loss:  28144.011962890625, valid loss:  28474.962890625\n",
      "epoch: 287 train loss:  28124.509033203125, valid loss:  28360.09375\n",
      "epoch: 288 train loss:  28164.57958984375, valid loss:  28373.05078125\n",
      "epoch: 289 train loss:  28106.896728515625, valid loss:  28272.126953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 290 train loss:  28115.984130859375, valid loss:  28390.76171875\n",
      "epoch: 291 train loss:  28072.89013671875, valid loss:  28313.83984375\n",
      "epoch: 292 train loss:  28088.903076171875, valid loss:  28386.671875\n",
      "epoch: 293 train loss:  28058.240478515625, valid loss:  28255.794921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 294 train loss:  28070.19775390625, valid loss:  28307.66015625\n",
      "epoch: 295 train loss:  28029.46728515625, valid loss:  28241.9375\n",
      "patience is reset, best model is updated\n",
      "epoch: 296 train loss:  28051.64697265625, valid loss:  28310.01953125\n",
      "epoch: 297 train loss:  28013.417724609375, valid loss:  28242.884765625\n",
      "epoch: 298 train loss:  28047.7568359375, valid loss:  28287.201171875\n",
      "epoch: 299 train loss:  28007.791015625, valid loss:  28206.625\n",
      "patience is reset, best model is updated\n",
      "epoch: 300 train loss:  28041.49365234375, valid loss:  28253.349609375\n",
      "epoch: 301 train loss:  27990.134521484375, valid loss:  28197.498046875\n",
      "patience is reset, best model is updated\n",
      "epoch: 302 train loss:  28020.289794921875, valid loss:  28266.427734375\n",
      "epoch: 303 train loss:  27981.104248046875, valid loss:  28233.396484375\n",
      "epoch: 304 train loss:  28024.9580078125, valid loss:  28269.380859375\n",
      "epoch: 305 train loss:  27977.506103515625, valid loss:  28209.20703125\n",
      "epoch: 306 train loss:  28010.319580078125, valid loss:  28211.251953125\n",
      "epoch: 307 train loss:  27957.330078125, valid loss:  28189.12890625\n",
      "patience is reset, best model is updated\n",
      "epoch: 308 train loss:  27999.98583984375, valid loss:  28180.298828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 309 train loss:  27947.095458984375, valid loss:  28172.37890625\n",
      "patience is reset, best model is updated\n",
      "epoch: 310 train loss:  27983.941162109375, valid loss:  28181.634765625\n",
      "epoch: 311 train loss:  27922.63671875, valid loss:  28163.751953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 312 train loss:  27953.346435546875, valid loss:  28140.462890625\n",
      "patience is reset, best model is updated\n",
      "epoch: 313 train loss:  27898.9462890625, valid loss:  28115.595703125\n",
      "patience is reset, best model is updated\n",
      "epoch: 314 train loss:  27929.297119140625, valid loss:  28116.158203125\n",
      "epoch: 315 train loss:  27880.605712890625, valid loss:  28077.80078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 316 train loss:  27916.174560546875, valid loss:  28115.4453125\n",
      "epoch: 317 train loss:  27869.78173828125, valid loss:  28064.107421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 318 train loss:  27902.96875, valid loss:  28124.876953125\n",
      "epoch: 319 train loss:  27869.423583984375, valid loss:  28061.1953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 320 train loss:  27894.738525390625, valid loss:  28175.7109375\n",
      "epoch: 321 train loss:  27874.4970703125, valid loss:  28065.49609375\n",
      "epoch: 322 train loss:  27879.99169921875, valid loss:  28214.599609375\n",
      "epoch: 323 train loss:  27862.731689453125, valid loss:  28044.509765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 324 train loss:  27835.296630859375, valid loss:  28154.7421875\n",
      "epoch: 325 train loss:  27842.26220703125, valid loss:  28040.537109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 326 train loss:  27839.623046875, valid loss:  28058.03515625\n",
      "epoch: 327 train loss:  27834.33349609375, valid loss:  28104.283203125\n",
      "epoch: 328 train loss:  27872.252685546875, valid loss:  28036.78125\n",
      "patience is reset, best model is updated\n",
      "epoch: 329 train loss:  27839.44384765625, valid loss:  28159.666015625\n",
      "epoch: 330 train loss:  27885.290283203125, valid loss:  28014.97265625\n",
      "patience is reset, best model is updated\n",
      "epoch: 331 train loss:  27838.510009765625, valid loss:  28181.357421875\n",
      "epoch: 332 train loss:  27888.016845703125, valid loss:  28002.74609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 333 train loss:  27820.45654296875, valid loss:  28176.015625\n",
      "epoch: 334 train loss:  27857.776611328125, valid loss:  28002.73046875\n",
      "patience is reset, best model is updated\n",
      "epoch: 335 train loss:  27795.037353515625, valid loss:  28162.142578125\n",
      "epoch: 336 train loss:  27828.7236328125, valid loss:  28028.4609375\n",
      "epoch: 337 train loss:  27778.4013671875, valid loss:  28166.79296875\n",
      "epoch: 338 train loss:  27810.88720703125, valid loss:  28091.189453125\n",
      "epoch: 339 train loss:  27768.010009765625, valid loss:  28187.732421875\n",
      "epoch: 340 train loss:  27790.496337890625, valid loss:  28164.6171875\n",
      "epoch: 341 train loss:  27768.369140625, valid loss:  28245.177734375\n",
      "epoch: 342 train loss:  27793.82275390625, valid loss:  28294.478515625\n",
      "epoch: 343 train loss:  27782.78369140625, valid loss:  28360.798828125\n",
      "epoch: 344 train loss:  27816.015869140625, valid loss:  28449.34765625\n",
      "epoch: 345 train loss:  27822.52099609375, valid loss:  28526.755859375\n",
      "epoch: 346 train loss:  27877.825927734375, valid loss:  28487.201171875\n",
      "epoch: 347 train loss:  27933.22314453125, valid loss:  28467.78515625\n",
      "epoch: 348 train loss:  28043.678955078125, valid loss:  28118.291015625\n",
      "epoch: 349 train loss:  28056.452392578125, valid loss:  28162.84375\n",
      "epoch: 350 train loss:  28113.568115234375, valid loss:  28759.326171875\n",
      "epoch: 351 train loss:  28023.243408203125, valid loss:  28469.8046875\n",
      "epoch: 352 train loss:  28057.375732421875, valid loss:  28231.876953125\n",
      "epoch: 353 train loss:  27942.2451171875, valid loss:  28407.380859375\n",
      "epoch: 354 train loss:  27875.96826171875, valid loss:  28267.650390625\n",
      "epoch: 355 train loss:  27849.743408203125, valid loss:  28233.626953125\n",
      "epoch: 356 train loss:  27912.8876953125, valid loss:  28319.634765625\n",
      "epoch: 357 train loss:  27845.920654296875, valid loss:  28292.609375\n",
      "epoch: 358 train loss:  27892.3583984375, valid loss:  28385.775390625\n",
      "epoch: 359 train loss:  27822.48876953125, valid loss:  28281.140625\n",
      "epoch: 360 train loss:  27853.0400390625, valid loss:  28347.365234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 361 train loss:  27832.513916015625, valid loss:  28399.880859375\n",
      "epoch: 362 train loss:  27897.90283203125, valid loss:  28479.3515625\n",
      "epoch: 363 train loss:  27909.208740234375, valid loss:  28345.43359375\n",
      "epoch: 364 train loss:  28039.55908203125, valid loss:  28110.09375\n",
      "epoch: 365 train loss:  28095.181396484375, valid loss:  28015.693359375\n",
      "epoch: 366 train loss:  28246.467529296875, valid loss:  28453.505859375\n",
      "epoch: 367 train loss:  28186.36767578125, valid loss:  28902.546875\n",
      "epoch: 368 train loss:  28272.891357421875, valid loss:  29415.322265625\n",
      "epoch: 369 train loss:  28332.50634765625, valid loss:  28774.591796875\n",
      "epoch: 370 train loss:  28214.029541015625, valid loss:  27985.595703125\n",
      "patience is reset, best model is updated\n",
      "epoch: 371 train loss:  27880.157958984375, valid loss:  28114.568359375\n",
      "epoch: 372 train loss:  27829.22412109375, valid loss:  28011.6328125\n",
      "epoch: 373 train loss:  27754.68505859375, valid loss:  28015.314453125\n",
      "epoch: 374 train loss:  27787.51513671875, valid loss:  28010.916015625\n",
      "epoch: 375 train loss:  27711.05810546875, valid loss:  27998.525390625\n",
      "epoch: 376 train loss:  27751.27001953125, valid loss:  28114.326171875\n",
      "epoch: 377 train loss:  27693.759033203125, valid loss:  27898.083984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 378 train loss:  27681.7646484375, valid loss:  27916.8203125\n",
      "epoch: 379 train loss:  27613.517822265625, valid loss:  27876.1796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 380 train loss:  27599.73828125, valid loss:  27814.21484375\n",
      "patience is reset, best model is updated\n",
      "epoch: 381 train loss:  27556.975830078125, valid loss:  27797.658203125\n",
      "patience is reset, best model is updated\n",
      "epoch: 382 train loss:  27576.737548828125, valid loss:  27764.498046875\n",
      "patience is reset, best model is updated\n",
      "epoch: 383 train loss:  27542.4677734375, valid loss:  27741.27734375\n",
      "patience is reset, best model is updated\n",
      "epoch: 384 train loss:  27576.752197265625, valid loss:  27812.146484375\n",
      "epoch: 385 train loss:  27530.146240234375, valid loss:  27760.650390625\n",
      "epoch: 386 train loss:  27567.71484375, valid loss:  27908.966796875\n",
      "epoch: 387 train loss:  27526.701416015625, valid loss:  27809.7109375\n",
      "epoch: 388 train loss:  27564.6533203125, valid loss:  27933.984375\n",
      "epoch: 389 train loss:  27528.278076171875, valid loss:  27797.107421875\n",
      "epoch: 390 train loss:  27571.558349609375, valid loss:  27880.5703125\n",
      "epoch: 391 train loss:  27524.950439453125, valid loss:  27752.095703125\n",
      "epoch: 392 train loss:  27558.059814453125, valid loss:  27824.23046875\n",
      "epoch: 393 train loss:  27507.58837890625, valid loss:  27749.1875\n",
      "epoch: 394 train loss:  27533.865234375, valid loss:  27810.818359375\n",
      "epoch: 395 train loss:  27488.312744140625, valid loss:  27754.33984375\n",
      "epoch: 396 train loss:  27514.849853515625, valid loss:  27819.90625\n",
      "epoch: 397 train loss:  27483.200439453125, valid loss:  27785.5078125\n",
      "epoch: 398 train loss:  27518.510009765625, valid loss:  27813.857421875\n",
      "epoch: 399 train loss:  27479.921875, valid loss:  27785.392578125\n",
      "epoch: 400 train loss:  27516.975830078125, valid loss:  27822.30078125\n",
      "epoch: 401 train loss:  27472.8037109375, valid loss:  27747.388671875\n",
      "epoch: 402 train loss:  27512.830322265625, valid loss:  27834.2421875\n",
      "epoch: 403 train loss:  27467.499267578125, valid loss:  27730.96875\n",
      "patience is reset, best model is updated\n",
      "epoch: 404 train loss:  27510.27197265625, valid loss:  27879.201171875\n",
      "epoch: 405 train loss:  27462.882568359375, valid loss:  27752.26953125\n",
      "epoch: 406 train loss:  27521.19384765625, valid loss:  27940.763671875\n",
      "epoch: 407 train loss:  27470.150146484375, valid loss:  27800.822265625\n",
      "epoch: 408 train loss:  27525.801513671875, valid loss:  27997.267578125\n",
      "epoch: 409 train loss:  27471.186767578125, valid loss:  27797.240234375\n",
      "epoch: 410 train loss:  27523.101806640625, valid loss:  28007.751953125\n",
      "epoch: 411 train loss:  27473.989990234375, valid loss:  27805.462890625\n",
      "epoch: 412 train loss:  27512.336181640625, valid loss:  27977.775390625\n",
      "epoch: 413 train loss:  27471.52001953125, valid loss:  27854.009765625\n",
      "epoch: 414 train loss:  27502.439208984375, valid loss:  27958.666015625\n",
      "epoch: 415 train loss:  27469.09814453125, valid loss:  27906.99609375\n",
      "epoch: 416 train loss:  27496.393798828125, valid loss:  27962.85546875\n",
      "epoch: 417 train loss:  27473.330078125, valid loss:  27931.2578125\n",
      "epoch: 418 train loss:  27512.78515625, valid loss:  27991.142578125\n",
      "epoch: 419 train loss:  27504.8544921875, valid loss:  27960.318359375\n",
      "epoch: 420 train loss:  27576.79248046875, valid loss:  27937.021484375\n",
      "epoch: 421 train loss:  27565.546875, valid loss:  27802.482421875\n",
      "epoch: 422 train loss:  27662.8076171875, valid loss:  27922.072265625\n",
      "epoch: 423 train loss:  27642.89404296875, valid loss:  27983.318359375\n",
      "epoch: 424 train loss:  27769.09765625, valid loss:  27953.86328125\n",
      "epoch: 425 train loss:  27768.06005859375, valid loss:  27967.15234375\n",
      "epoch: 426 train loss:  27794.991943359375, valid loss:  28262.41015625\n",
      "epoch: 427 train loss:  27824.92529296875, valid loss:  28357.69921875\n",
      "epoch: 428 train loss:  27886.5556640625, valid loss:  28170.134765625\n",
      "epoch: 429 train loss:  27759.227294921875, valid loss:  27797.1875\n",
      "epoch: 430 train loss:  27669.761474609375, valid loss:  27688.458984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 431 train loss:  27502.468505859375, valid loss:  27630.486328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 432 train loss:  27508.58203125, valid loss:  27734.69921875\n",
      "epoch: 433 train loss:  27412.807861328125, valid loss:  27623.41796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 434 train loss:  27436.190185546875, valid loss:  27611.228515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 435 train loss:  27392.036865234375, valid loss:  27600.82421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 436 train loss:  27395.35791015625, valid loss:  27597.884765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 437 train loss:  27361.61083984375, valid loss:  27638.2890625\n",
      "epoch: 438 train loss:  27396.908447265625, valid loss:  27687.857421875\n",
      "epoch: 439 train loss:  27367.659423828125, valid loss:  27694.373046875\n",
      "epoch: 440 train loss:  27393.23681640625, valid loss:  27694.216796875\n",
      "epoch: 441 train loss:  27355.586669921875, valid loss:  27670.78515625\n",
      "epoch: 442 train loss:  27391.974609375, valid loss:  27726.193359375\n",
      "epoch: 443 train loss:  27342.160400390625, valid loss:  27602.873046875\n",
      "epoch: 444 train loss:  27392.587158203125, valid loss:  27651.978515625\n",
      "epoch: 445 train loss:  27342.68896484375, valid loss:  27568.01953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 446 train loss:  27374.767822265625, valid loss:  27566.732421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 447 train loss:  27314.899658203125, valid loss:  27494.51171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 448 train loss:  27341.30712890625, valid loss:  27498.1875\n",
      "epoch: 449 train loss:  27291.452880859375, valid loss:  27481.193359375\n",
      "patience is reset, best model is updated\n",
      "epoch: 450 train loss:  27317.040283203125, valid loss:  27489.427734375\n",
      "epoch: 451 train loss:  27282.576904296875, valid loss:  27563.880859375\n",
      "epoch: 452 train loss:  27316.513916015625, valid loss:  27554.23046875\n",
      "epoch: 453 train loss:  27289.438232421875, valid loss:  27660.564453125\n",
      "epoch: 454 train loss:  27322.60205078125, valid loss:  27662.455078125\n",
      "epoch: 455 train loss:  27305.0341796875, valid loss:  27730.771484375\n",
      "epoch: 456 train loss:  27357.4638671875, valid loss:  27729.076171875\n",
      "epoch: 457 train loss:  27347.79052734375, valid loss:  27678.02734375\n",
      "epoch: 458 train loss:  27409.111083984375, valid loss:  27623.87109375\n",
      "epoch: 459 train loss:  27416.798095703125, valid loss:  27639.78125\n",
      "epoch: 460 train loss:  27516.19775390625, valid loss:  27704.310546875\n",
      "epoch: 461 train loss:  27465.0947265625, valid loss:  27831.876953125\n",
      "epoch: 462 train loss:  27555.77880859375, valid loss:  27976.12109375\n",
      "epoch: 463 train loss:  27483.59130859375, valid loss:  27806.630859375\n",
      "epoch: 464 train loss:  27505.5380859375, valid loss:  27647.521484375\n",
      "epoch: 465 train loss:  27428.1533203125, valid loss:  27648.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 466 train loss:  27403.558837890625, valid loss:  27586.365234375\n",
      "epoch: 467 train loss:  27349.786865234375, valid loss:  27725.39453125\n",
      "epoch: 468 train loss:  27353.439697265625, valid loss:  27520.673828125\n",
      "epoch: 469 train loss:  27301.552978515625, valid loss:  27473.09765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 470 train loss:  27310.22607421875, valid loss:  27431.25390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 471 train loss:  27228.037353515625, valid loss:  27465.328125\n",
      "epoch: 472 train loss:  27253.164794921875, valid loss:  27570.8515625\n",
      "epoch: 473 train loss:  27205.967529296875, valid loss:  27484.04296875\n",
      "epoch: 474 train loss:  27238.478515625, valid loss:  27494.455078125\n",
      "epoch: 475 train loss:  27200.7060546875, valid loss:  27408.36328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 476 train loss:  27215.036865234375, valid loss:  27391.5078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 477 train loss:  27184.72705078125, valid loss:  27405.064453125\n",
      "epoch: 478 train loss:  27194.1748046875, valid loss:  27371.994140625\n",
      "patience is reset, best model is updated\n",
      "epoch: 479 train loss:  27169.928466796875, valid loss:  27430.181640625\n",
      "epoch: 480 train loss:  27201.395263671875, valid loss:  27357.7421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 481 train loss:  27172.6630859375, valid loss:  27392.017578125\n",
      "epoch: 482 train loss:  27205.742431640625, valid loss:  27351.8203125\n",
      "patience is reset, best model is updated\n",
      "epoch: 483 train loss:  27161.2197265625, valid loss:  27360.6171875\n",
      "epoch: 484 train loss:  27196.692626953125, valid loss:  27372.408203125\n",
      "epoch: 485 train loss:  27153.68115234375, valid loss:  27365.49609375\n",
      "epoch: 486 train loss:  27192.411376953125, valid loss:  27423.068359375\n",
      "epoch: 487 train loss:  27161.630859375, valid loss:  27388.263671875\n",
      "epoch: 488 train loss:  27205.68994140625, valid loss:  27479.7109375\n",
      "epoch: 489 train loss:  27173.0712890625, valid loss:  27390.958984375\n",
      "epoch: 490 train loss:  27261.3935546875, valid loss:  27485.529296875\n",
      "epoch: 491 train loss:  27214.1103515625, valid loss:  27431.5703125\n",
      "epoch: 492 train loss:  27262.8408203125, valid loss:  27464.908203125\n",
      "epoch: 493 train loss:  27180.585205078125, valid loss:  27432.90625\n",
      "epoch: 494 train loss:  27196.645751953125, valid loss:  27423.935546875\n",
      "epoch: 495 train loss:  27146.911865234375, valid loss:  27423.958984375\n",
      "epoch: 496 train loss:  27160.841064453125, valid loss:  27389.044921875\n",
      "epoch: 497 train loss:  27122.06591796875, valid loss:  27432.05859375\n",
      "epoch: 498 train loss:  27138.37109375, valid loss:  27398.482421875\n",
      "epoch: 499 train loss:  27099.798828125, valid loss:  27423.830078125\n",
      "epoch: 500 train loss:  27125.912841796875, valid loss:  27421.658203125\n",
      "epoch: 501 train loss:  27097.777587890625, valid loss:  27448.322265625\n",
      "epoch: 502 train loss:  27121.673828125, valid loss:  27433.578125\n",
      "epoch: 503 train loss:  27094.441162109375, valid loss:  27461.931640625\n",
      "epoch: 504 train loss:  27122.22509765625, valid loss:  27433.310546875\n",
      "epoch: 505 train loss:  27110.32470703125, valid loss:  27465.294921875\n",
      "epoch: 506 train loss:  27144.618408203125, valid loss:  27467.619140625\n",
      "epoch: 507 train loss:  27105.944580078125, valid loss:  27420.04296875\n",
      "epoch: 508 train loss:  27163.637451171875, valid loss:  27507.34375\n",
      "epoch: 509 train loss:  27125.195068359375, valid loss:  27462.552734375\n",
      "epoch: 510 train loss:  27170.093017578125, valid loss:  27521.4296875\n",
      "epoch: 511 train loss:  27131.77392578125, valid loss:  27503.921875\n",
      "epoch: 512 train loss:  27186.125732421875, valid loss:  27666.189453125\n",
      "epoch: 513 train loss:  27138.5791015625, valid loss:  27480.97265625\n",
      "epoch: 514 train loss:  27216.2373046875, valid loss:  27654.59765625\n",
      "epoch: 515 train loss:  27154.8251953125, valid loss:  27512.296875\n",
      "epoch: 516 train loss:  27186.18310546875, valid loss:  27556.578125\n",
      "epoch: 517 train loss:  27131.168701171875, valid loss:  27443.982421875\n",
      "epoch: 518 train loss:  27163.912353515625, valid loss:  27473.22265625\n",
      "epoch: 519 train loss:  27130.778076171875, valid loss:  27408.658203125\n",
      "epoch: 520 train loss:  27156.727294921875, valid loss:  27394.806640625\n",
      "epoch: 521 train loss:  27138.1240234375, valid loss:  27370.271484375\n",
      "epoch: 522 train loss:  27157.06884765625, valid loss:  27366.515625\n",
      "epoch: 523 train loss:  27142.39501953125, valid loss:  27509.669921875\n",
      "epoch: 524 train loss:  27181.166015625, valid loss:  27708.001953125\n",
      "epoch: 525 train loss:  27212.74755859375, valid loss:  28062.978515625\n",
      "epoch: 526 train loss:  27304.205078125, valid loss:  28035.255859375\n",
      "epoch: 527 train loss:  27370.361083984375, valid loss:  27753.314453125\n",
      "epoch: 528 train loss:  27399.986083984375, valid loss:  27875.765625\n",
      "epoch: 529 train loss:  27333.23095703125, valid loss:  27607.39453125\n",
      "epoch: 530 train loss:  27408.625, valid loss:  27799.74609375\n",
      "epoch: 531 train loss:  27302.257568359375, valid loss:  27622.21875\n",
      "epoch: 532 train loss:  27350.763427734375, valid loss:  27422.1015625\n",
      "epoch: 533 train loss:  27230.255859375, valid loss:  27338.423828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 534 train loss:  27207.140625, valid loss:  27271.79296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 535 train loss:  27137.225341796875, valid loss:  27398.08984375\n",
      "epoch: 536 train loss:  27157.364013671875, valid loss:  27313.130859375\n",
      "epoch: 537 train loss:  27108.982177734375, valid loss:  27343.0625\n",
      "epoch: 538 train loss:  27131.30078125, valid loss:  27248.794921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 539 train loss:  27039.927978515625, valid loss:  27265.69140625\n",
      "epoch: 540 train loss:  27055.408935546875, valid loss:  27256.9453125\n",
      "epoch: 541 train loss:  26989.1328125, valid loss:  27239.134765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 542 train loss:  27010.622802734375, valid loss:  27293.55859375\n",
      "epoch: 543 train loss:  26959.956298828125, valid loss:  27246.58203125\n",
      "epoch: 544 train loss:  26982.26025390625, valid loss:  27284.236328125\n",
      "epoch: 545 train loss:  26950.074462890625, valid loss:  27229.4765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 546 train loss:  26965.7578125, valid loss:  27262.9921875\n",
      "epoch: 547 train loss:  26935.403564453125, valid loss:  27214.41796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 548 train loss:  26961.693603515625, valid loss:  27238.30078125\n",
      "epoch: 549 train loss:  26930.1796875, valid loss:  27208.11328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 550 train loss:  26953.9853515625, valid loss:  27236.673828125\n",
      "epoch: 551 train loss:  26924.241455078125, valid loss:  27222.29296875\n",
      "epoch: 552 train loss:  26948.84423828125, valid loss:  27259.447265625\n",
      "epoch: 553 train loss:  26929.548828125, valid loss:  27260.388671875\n",
      "epoch: 554 train loss:  26949.279296875, valid loss:  27266.076171875\n",
      "epoch: 555 train loss:  26939.192626953125, valid loss:  27343.33203125\n",
      "epoch: 556 train loss:  26982.44580078125, valid loss:  27301.119140625\n",
      "epoch: 557 train loss:  26950.763671875, valid loss:  27356.3515625\n",
      "epoch: 558 train loss:  27005.04931640625, valid loss:  27396.552734375\n",
      "epoch: 559 train loss:  26974.408935546875, valid loss:  27368.76171875\n",
      "epoch: 560 train loss:  27005.927490234375, valid loss:  27372.70703125\n",
      "epoch: 561 train loss:  26990.376220703125, valid loss:  27430.123046875\n",
      "epoch: 562 train loss:  27059.26708984375, valid loss:  27376.859375\n",
      "epoch: 563 train loss:  27004.548828125, valid loss:  27307.47265625\n",
      "epoch: 564 train loss:  27084.490478515625, valid loss:  27404.724609375\n",
      "epoch: 565 train loss:  27024.50732421875, valid loss:  27244.263671875\n",
      "epoch: 566 train loss:  27063.208984375, valid loss:  27369.8125\n",
      "epoch: 567 train loss:  27017.192138671875, valid loss:  27286.904296875\n",
      "epoch: 568 train loss:  27070.994140625, valid loss:  27478.947265625\n",
      "epoch: 569 train loss:  27025.376708984375, valid loss:  27388.326171875\n",
      "epoch: 570 train loss:  27094.8203125, valid loss:  27584.357421875\n",
      "epoch: 571 train loss:  27057.633056640625, valid loss:  27422.908203125\n",
      "epoch: 572 train loss:  27104.470947265625, valid loss:  27477.26171875\n",
      "epoch: 573 train loss:  27070.720458984375, valid loss:  27396.3203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 574 train loss:  27073.480712890625, valid loss:  27484.7265625\n",
      "epoch: 575 train loss:  27061.320556640625, valid loss:  27608.255859375\n",
      "epoch: 576 train loss:  27093.017333984375, valid loss:  27567.73828125\n",
      "epoch: 577 train loss:  27103.1435546875, valid loss:  27484.826171875\n",
      "epoch: 578 train loss:  27117.17431640625, valid loss:  27224.34765625\n",
      "epoch: 579 train loss:  26998.07470703125, valid loss:  27220.1171875\n",
      "epoch: 580 train loss:  26983.991943359375, valid loss:  27419.982421875\n",
      "epoch: 581 train loss:  26933.05517578125, valid loss:  27325.0234375\n",
      "epoch: 582 train loss:  26955.022216796875, valid loss:  27301.685546875\n",
      "epoch: 583 train loss:  26927.897216796875, valid loss:  27266.94921875\n",
      "epoch: 584 train loss:  26927.389404296875, valid loss:  27219.765625\n",
      "epoch: 585 train loss:  26933.888427734375, valid loss:  27409.83203125\n",
      "epoch: 586 train loss:  26968.064453125, valid loss:  27212.470703125\n",
      "epoch: 587 train loss:  26941.87744140625, valid loss:  27265.072265625\n",
      "epoch: 588 train loss:  26992.436767578125, valid loss:  27214.341796875\n",
      "epoch: 589 train loss:  26928.9140625, valid loss:  27187.357421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 590 train loss:  26983.521240234375, valid loss:  27247.951171875\n",
      "epoch: 591 train loss:  26924.1162109375, valid loss:  27182.05859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 592 train loss:  26977.648681640625, valid loss:  27273.0390625\n",
      "epoch: 593 train loss:  26920.041015625, valid loss:  27254.998046875\n",
      "epoch: 594 train loss:  26942.459716796875, valid loss:  27281.5859375\n",
      "epoch: 595 train loss:  26907.32666015625, valid loss:  27262.841796875\n",
      "epoch: 596 train loss:  26925.2216796875, valid loss:  27323.07421875\n",
      "epoch: 597 train loss:  26887.451416015625, valid loss:  27289.91015625\n",
      "epoch: 598 train loss:  26941.33154296875, valid loss:  27341.162109375\n",
      "epoch: 599 train loss:  26909.685546875, valid loss:  27269.759765625\n",
      "epoch: 600 train loss:  26982.02880859375, valid loss:  27269.775390625\n",
      "epoch: 601 train loss:  26940.89697265625, valid loss:  27191.52734375\n",
      "epoch: 602 train loss:  26948.31591796875, valid loss:  27201.16015625\n",
      "epoch: 603 train loss:  26898.6650390625, valid loss:  27198.962890625\n",
      "epoch: 604 train loss:  26912.739013671875, valid loss:  27127.53515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 605 train loss:  26864.677978515625, valid loss:  27157.267578125\n",
      "epoch: 606 train loss:  26884.79736328125, valid loss:  27115.263671875\n",
      "patience is reset, best model is updated\n",
      "epoch: 607 train loss:  26836.417236328125, valid loss:  27176.392578125\n",
      "epoch: 608 train loss:  26863.3994140625, valid loss:  27168.57421875\n",
      "epoch: 609 train loss:  26826.01025390625, valid loss:  27223.107421875\n",
      "epoch: 610 train loss:  26854.349365234375, valid loss:  27205.1171875\n",
      "epoch: 611 train loss:  26828.39990234375, valid loss:  27271.671875\n",
      "epoch: 612 train loss:  26859.406494140625, valid loss:  27225.40234375\n",
      "epoch: 613 train loss:  26829.20263671875, valid loss:  27272.810546875\n",
      "epoch: 614 train loss:  26864.108642578125, valid loss:  27300.970703125\n",
      "epoch: 615 train loss:  26837.618896484375, valid loss:  27310.751953125\n",
      "epoch: 616 train loss:  26875.879638671875, valid loss:  27340.044921875\n",
      "epoch: 617 train loss:  26844.496337890625, valid loss:  27272.044921875\n",
      "epoch: 618 train loss:  26886.756103515625, valid loss:  27338.58203125\n",
      "epoch: 619 train loss:  26851.92822265625, valid loss:  27220.703125\n",
      "epoch: 620 train loss:  26900.6640625, valid loss:  27263.734375\n",
      "epoch: 621 train loss:  26876.33984375, valid loss:  27145.92578125\n",
      "epoch: 622 train loss:  26912.92724609375, valid loss:  27208.501953125\n",
      "epoch: 623 train loss:  26881.642333984375, valid loss:  27231.009765625\n",
      "epoch: 624 train loss:  26909.419921875, valid loss:  27360.76953125\n",
      "epoch: 625 train loss:  26883.900634765625, valid loss:  27376.68359375\n",
      "epoch: 626 train loss:  26944.208251953125, valid loss:  27609.26953125\n",
      "epoch: 627 train loss:  26913.134033203125, valid loss:  27511.306640625\n",
      "epoch: 628 train loss:  26981.0380859375, valid loss:  27587.419921875\n",
      "epoch: 629 train loss:  26960.90673828125, valid loss:  27302.615234375\n",
      "epoch: 630 train loss:  26998.217041015625, valid loss:  27198.412109375\n",
      "epoch: 631 train loss:  26952.230712890625, valid loss:  27237.009765625\n",
      "epoch: 632 train loss:  26915.23779296875, valid loss:  27292.927734375\n",
      "epoch: 633 train loss:  26907.05517578125, valid loss:  27433.025390625\n",
      "epoch: 634 train loss:  26927.902099609375, valid loss:  27369.14453125\n",
      "epoch: 635 train loss:  26887.076904296875, valid loss:  27391.5234375\n",
      "epoch: 636 train loss:  26921.761962890625, valid loss:  27310.916015625\n",
      "epoch: 637 train loss:  26880.67333984375, valid loss:  27170.8984375\n",
      "epoch: 638 train loss:  26889.8017578125, valid loss:  27138.037109375\n",
      "epoch: 639 train loss:  26843.86083984375, valid loss:  27290.61328125\n",
      "epoch: 640 train loss:  26870.581787109375, valid loss:  27302.984375\n",
      "epoch: 641 train loss:  26828.771484375, valid loss:  27177.228515625\n",
      "epoch: 642 train loss:  26866.325927734375, valid loss:  27282.48046875\n",
      "epoch: 643 train loss:  26798.795654296875, valid loss:  27113.287109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 644 train loss:  26820.6044921875, valid loss:  27111.13671875\n",
      "patience is reset, best model is updated\n",
      "epoch: 645 train loss:  26780.36474609375, valid loss:  27146.98828125\n",
      "epoch: 646 train loss:  26776.80322265625, valid loss:  27096.318359375\n",
      "patience is reset, best model is updated\n",
      "epoch: 647 train loss:  26762.482666015625, valid loss:  27212.8828125\n",
      "epoch: 648 train loss:  26777.1943359375, valid loss:  27093.4296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 649 train loss:  26748.826904296875, valid loss:  27148.517578125\n",
      "epoch: 650 train loss:  26783.030029296875, valid loss:  27138.400390625\n",
      "epoch: 651 train loss:  26744.941650390625, valid loss:  27080.673828125\n",
      "patience is reset, best model is updated\n",
      "epoch: 652 train loss:  26792.835205078125, valid loss:  27159.5859375\n",
      "epoch: 653 train loss:  26759.225341796875, valid loss:  27100.638671875\n",
      "epoch: 654 train loss:  26793.99853515625, valid loss:  27188.125\n",
      "epoch: 655 train loss:  26764.98046875, valid loss:  27100.560546875\n",
      "epoch: 656 train loss:  26822.326904296875, valid loss:  27111.46875\n",
      "epoch: 657 train loss:  26778.801513671875, valid loss:  27067.611328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 658 train loss:  26835.17822265625, valid loss:  27202.287109375\n",
      "epoch: 659 train loss:  26778.53515625, valid loss:  27138.28125\n",
      "epoch: 660 train loss:  26902.760009765625, valid loss:  27266.6015625\n",
      "epoch: 661 train loss:  27084.4189453125, valid loss:  27349.96484375\n",
      "epoch: 662 train loss:  27348.08642578125, valid loss:  27160.671875\n",
      "epoch: 663 train loss:  26948.011474609375, valid loss:  27162.283203125\n",
      "epoch: 664 train loss:  26930.248779296875, valid loss:  27168.546875\n",
      "epoch: 665 train loss:  26775.349365234375, valid loss:  27054.875\n",
      "patience is reset, best model is updated\n",
      "epoch: 666 train loss:  26739.231689453125, valid loss:  27028.76953125\n",
      "patience is reset, best model is updated\n",
      "epoch: 667 train loss:  26709.416259765625, valid loss:  27011.564453125\n",
      "patience is reset, best model is updated\n",
      "epoch: 668 train loss:  26715.03173828125, valid loss:  26996.005859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 669 train loss:  26693.035888671875, valid loss:  27005.173828125\n",
      "epoch: 670 train loss:  26689.007080078125, valid loss:  26963.537109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 671 train loss:  26673.330322265625, valid loss:  26995.96484375\n",
      "epoch: 672 train loss:  26666.9013671875, valid loss:  26959.951171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 673 train loss:  26649.53466796875, valid loss:  26988.396484375\n",
      "epoch: 674 train loss:  26645.18359375, valid loss:  26997.080078125\n",
      "epoch: 675 train loss:  26635.014404296875, valid loss:  27012.9296875\n",
      "epoch: 676 train loss:  26640.525634765625, valid loss:  27044.03125\n",
      "epoch: 677 train loss:  26627.71533203125, valid loss:  26994.90625\n",
      "epoch: 678 train loss:  26637.26708984375, valid loss:  27000.935546875\n",
      "epoch: 679 train loss:  26624.3095703125, valid loss:  26936.21875\n",
      "patience is reset, best model is updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 680 train loss:  26633.278564453125, valid loss:  26936.0078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 681 train loss:  26624.814697265625, valid loss:  26924.978515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 682 train loss:  26634.370849609375, valid loss:  26940.466796875\n",
      "epoch: 683 train loss:  26638.2080078125, valid loss:  26954.09765625\n",
      "epoch: 684 train loss:  26659.86962890625, valid loss:  26952.28515625\n",
      "epoch: 685 train loss:  26664.81005859375, valid loss:  26934.626953125\n",
      "epoch: 686 train loss:  26703.418212890625, valid loss:  26975.017578125\n",
      "epoch: 687 train loss:  26691.58154296875, valid loss:  26952.275390625\n",
      "epoch: 688 train loss:  26754.89404296875, valid loss:  27120.158203125\n",
      "epoch: 689 train loss:  26723.125244140625, valid loss:  27068.125\n",
      "epoch: 690 train loss:  26796.17578125, valid loss:  27256.404296875\n",
      "epoch: 691 train loss:  26761.58935546875, valid loss:  27143.8984375\n",
      "epoch: 692 train loss:  26820.651123046875, valid loss:  27264.7421875\n",
      "epoch: 693 train loss:  26788.677001953125, valid loss:  27229.8046875\n",
      "epoch: 694 train loss:  26845.241943359375, valid loss:  27372.12109375\n",
      "epoch: 695 train loss:  26831.6005859375, valid loss:  27400.40234375\n",
      "epoch: 696 train loss:  26912.163330078125, valid loss:  27552.474609375\n",
      "epoch: 697 train loss:  26906.437255859375, valid loss:  27618.123046875\n",
      "epoch: 698 train loss:  27003.088134765625, valid loss:  27916.810546875\n",
      "epoch: 699 train loss:  27059.6796875, valid loss:  27986.8515625\n",
      "epoch: 700 train loss:  27169.497314453125, valid loss:  28067.396484375\n",
      "epoch: 701 train loss:  27266.38037109375, valid loss:  28141.357421875\n",
      "epoch: 702 train loss:  27352.34814453125, valid loss:  27269.990234375\n",
      "epoch: 703 train loss:  27136.607177734375, valid loss:  27865.365234375\n",
      "epoch: 704 train loss:  27108.206787109375, valid loss:  27434.79296875\n",
      "epoch: 705 train loss:  26945.246337890625, valid loss:  27352.328125\n",
      "epoch: 706 train loss:  26845.929931640625, valid loss:  27013.0078125\n",
      "epoch: 707 train loss:  26738.535400390625, valid loss:  27112.11328125\n",
      "epoch: 708 train loss:  26755.935302734375, valid loss:  27184.33203125\n",
      "epoch: 709 train loss:  26717.761962890625, valid loss:  27098.78515625\n",
      "epoch: 710 train loss:  26753.71875, valid loss:  27068.271484375\n",
      "epoch: 711 train loss:  26721.993408203125, valid loss:  26924.86328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 712 train loss:  26751.336181640625, valid loss:  27037.302734375\n",
      "epoch: 713 train loss:  26676.3369140625, valid loss:  26986.62109375\n",
      "epoch: 714 train loss:  26680.75830078125, valid loss:  27024.947265625\n",
      "epoch: 715 train loss:  26640.04248046875, valid loss:  26984.1484375\n",
      "epoch: 716 train loss:  26658.331787109375, valid loss:  26989.044921875\n",
      "epoch: 717 train loss:  26617.646240234375, valid loss:  26987.1640625\n",
      "epoch: 718 train loss:  26633.20556640625, valid loss:  27010.4921875\n",
      "epoch: 719 train loss:  26595.687744140625, valid loss:  26951.369140625\n",
      "epoch: 720 train loss:  26614.666748046875, valid loss:  26968.53515625\n",
      "epoch: 721 train loss:  26580.3515625, valid loss:  26901.34765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 722 train loss:  26598.819091796875, valid loss:  26958.6328125\n",
      "epoch: 723 train loss:  26565.021484375, valid loss:  26918.390625\n",
      "epoch: 724 train loss:  26589.869873046875, valid loss:  27014.16015625\n",
      "epoch: 725 train loss:  26563.21337890625, valid loss:  26959.837890625\n",
      "epoch: 726 train loss:  26592.628173828125, valid loss:  27035.365234375\n",
      "epoch: 727 train loss:  26565.625732421875, valid loss:  26971.421875\n",
      "epoch: 728 train loss:  26598.362548828125, valid loss:  27074.634765625\n",
      "epoch: 729 train loss:  26571.427001953125, valid loss:  26991.830078125\n",
      "epoch: 730 train loss:  26613.240478515625, valid loss:  27118.490234375\n",
      "epoch: 731 train loss:  26581.75634765625, valid loss:  26983.4453125\n",
      "epoch: 732 train loss:  26623.107666015625, valid loss:  27096.486328125\n",
      "epoch: 733 train loss:  26590.992919921875, valid loss:  26942.525390625\n",
      "epoch: 734 train loss:  26624.196533203125, valid loss:  27004.091796875\n",
      "epoch: 735 train loss:  26591.902099609375, valid loss:  26902.748046875\n",
      "epoch: 736 train loss:  26618.93017578125, valid loss:  27003.22265625\n",
      "epoch: 737 train loss:  26593.26318359375, valid loss:  26896.921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 738 train loss:  26623.196044921875, valid loss:  26999.443359375\n",
      "epoch: 739 train loss:  26597.376708984375, valid loss:  26925.732421875\n",
      "epoch: 740 train loss:  26626.154296875, valid loss:  27058.6328125\n",
      "epoch: 741 train loss:  26608.200927734375, valid loss:  27039.662109375\n",
      "epoch: 742 train loss:  26631.63134765625, valid loss:  27164.0078125\n",
      "epoch: 743 train loss:  26603.82568359375, valid loss:  27136.4296875\n",
      "epoch: 744 train loss:  26617.625732421875, valid loss:  27106.39453125\n",
      "epoch: 745 train loss:  26586.81689453125, valid loss:  26973.828125\n",
      "epoch: 746 train loss:  26605.44677734375, valid loss:  26974.91796875\n",
      "epoch: 747 train loss:  26581.616943359375, valid loss:  26908.6796875\n",
      "epoch: 748 train loss:  26608.72021484375, valid loss:  26935.17578125\n",
      "epoch: 749 train loss:  26583.262939453125, valid loss:  26887.755859375\n",
      "patience is reset, best model is updated\n",
      "epoch: 750 train loss:  26611.279541015625, valid loss:  26922.33203125\n",
      "epoch: 751 train loss:  26573.30029296875, valid loss:  26918.7421875\n",
      "epoch: 752 train loss:  26602.327880859375, valid loss:  27025.236328125\n",
      "epoch: 753 train loss:  26572.9150390625, valid loss:  27053.93359375\n",
      "epoch: 754 train loss:  26629.0146484375, valid loss:  27131.404296875\n",
      "epoch: 755 train loss:  26599.953857421875, valid loss:  27031.87890625\n",
      "epoch: 756 train loss:  26650.1552734375, valid loss:  27084.416015625\n",
      "epoch: 757 train loss:  26601.390869140625, valid loss:  26923.869140625\n",
      "epoch: 758 train loss:  26623.173583984375, valid loss:  26979.998046875\n",
      "epoch: 759 train loss:  26573.748046875, valid loss:  26879.662109375\n",
      "patience is reset, best model is updated\n",
      "epoch: 760 train loss:  26588.495361328125, valid loss:  26931.837890625\n",
      "epoch: 761 train loss:  26558.41796875, valid loss:  26867.93359375\n",
      "patience is reset, best model is updated\n",
      "epoch: 762 train loss:  26575.114013671875, valid loss:  26962.728515625\n",
      "epoch: 763 train loss:  26565.644775390625, valid loss:  27015.36328125\n",
      "epoch: 764 train loss:  26583.494140625, valid loss:  27058.3359375\n",
      "epoch: 765 train loss:  26558.606689453125, valid loss:  26966.1875\n",
      "epoch: 766 train loss:  26576.7958984375, valid loss:  26978.62890625\n",
      "epoch: 767 train loss:  26537.254638671875, valid loss:  26883.361328125\n",
      "epoch: 768 train loss:  26569.25927734375, valid loss:  26949.685546875\n",
      "epoch: 769 train loss:  26526.024658203125, valid loss:  26819.34765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 770 train loss:  26550.607421875, valid loss:  26868.251953125\n",
      "epoch: 771 train loss:  26504.907958984375, valid loss:  26826.103515625\n",
      "epoch: 772 train loss:  26526.90283203125, valid loss:  26866.107421875\n",
      "epoch: 773 train loss:  26499.241455078125, valid loss:  26862.595703125\n",
      "epoch: 774 train loss:  26529.859130859375, valid loss:  26871.537109375\n",
      "epoch: 775 train loss:  26498.41748046875, valid loss:  26881.634765625\n",
      "epoch: 776 train loss:  26533.349365234375, valid loss:  26923.50390625\n",
      "epoch: 777 train loss:  26497.16552734375, valid loss:  26866.455078125\n",
      "epoch: 778 train loss:  26534.065185546875, valid loss:  26888.3203125\n",
      "epoch: 779 train loss:  26488.506103515625, valid loss:  26829.56640625\n",
      "epoch: 780 train loss:  26514.839111328125, valid loss:  26904.88671875\n",
      "epoch: 781 train loss:  26468.94384765625, valid loss:  26818.978515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 782 train loss:  26501.37353515625, valid loss:  26873.03125\n",
      "epoch: 783 train loss:  26464.78955078125, valid loss:  26769.91015625\n",
      "patience is reset, best model is updated\n",
      "epoch: 784 train loss:  26495.3291015625, valid loss:  26830.455078125\n",
      "epoch: 785 train loss:  26466.588134765625, valid loss:  26790.890625\n",
      "epoch: 786 train loss:  26495.74072265625, valid loss:  26875.326171875\n",
      "epoch: 787 train loss:  26477.54833984375, valid loss:  26829.572265625\n",
      "epoch: 788 train loss:  26500.373046875, valid loss:  26903.181640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 789 train loss:  26475.039794921875, valid loss:  26899.51171875\n",
      "epoch: 790 train loss:  26493.4912109375, valid loss:  26960.705078125\n",
      "epoch: 791 train loss:  26475.897216796875, valid loss:  26919.59765625\n",
      "epoch: 792 train loss:  26496.208984375, valid loss:  26943.068359375\n",
      "epoch: 793 train loss:  26468.37060546875, valid loss:  26879.64453125\n",
      "epoch: 794 train loss:  26500.639404296875, valid loss:  26930.998046875\n",
      "epoch: 795 train loss:  26464.74169921875, valid loss:  26808.732421875\n",
      "epoch: 796 train loss:  26497.470947265625, valid loss:  26867.576171875\n",
      "epoch: 797 train loss:  26457.524658203125, valid loss:  26771.3359375\n",
      "epoch: 798 train loss:  26494.958251953125, valid loss:  26844.443359375\n",
      "epoch: 799 train loss:  26455.271728515625, valid loss:  26776.060546875\n",
      "epoch: 800 train loss:  26493.73291015625, valid loss:  26795.306640625\n",
      "epoch: 801 train loss:  26456.475341796875, valid loss:  26769.046875\n",
      "patience is reset, best model is updated\n",
      "epoch: 802 train loss:  26483.383544921875, valid loss:  26821.779296875\n",
      "epoch: 803 train loss:  26449.881103515625, valid loss:  26862.2265625\n",
      "epoch: 804 train loss:  26487.83056640625, valid loss:  26876.2890625\n",
      "epoch: 805 train loss:  26455.01806640625, valid loss:  26919.80078125\n",
      "epoch: 806 train loss:  26498.074462890625, valid loss:  26933.53515625\n",
      "epoch: 807 train loss:  26474.17919921875, valid loss:  26988.265625\n",
      "epoch: 808 train loss:  26516.846435546875, valid loss:  26947.875\n",
      "epoch: 809 train loss:  26491.664306640625, valid loss:  26907.818359375\n",
      "epoch: 810 train loss:  26530.428955078125, valid loss:  26906.2109375\n",
      "epoch: 811 train loss:  26477.802490234375, valid loss:  26826.7578125\n",
      "epoch: 812 train loss:  26527.6845703125, valid loss:  26934.064453125\n",
      "epoch: 813 train loss:  26477.919189453125, valid loss:  26835.626953125\n",
      "epoch: 814 train loss:  26515.116455078125, valid loss:  26961.1953125\n",
      "epoch: 815 train loss:  26472.40380859375, valid loss:  26852.609375\n",
      "epoch: 816 train loss:  26499.043701171875, valid loss:  26923.33984375\n",
      "epoch: 817 train loss:  26477.186279296875, valid loss:  26897.990234375\n",
      "epoch: 818 train loss:  26498.5283203125, valid loss:  26998.896484375\n",
      "epoch: 819 train loss:  26489.882080078125, valid loss:  27062.765625\n",
      "epoch: 820 train loss:  26503.442626953125, valid loss:  27040.712890625\n",
      "epoch: 821 train loss:  26496.979736328125, valid loss:  26906.650390625\n",
      "epoch: 822 train loss:  26512.5537109375, valid loss:  26831.962890625\n",
      "epoch: 823 train loss:  26456.765625, valid loss:  26795.57421875\n",
      "epoch: 824 train loss:  26473.777587890625, valid loss:  26894.861328125\n",
      "epoch: 825 train loss:  26421.70556640625, valid loss:  26802.96484375\n",
      "epoch: 826 train loss:  26443.92236328125, valid loss:  26805.05078125\n",
      "epoch: 827 train loss:  26412.736083984375, valid loss:  26768.732421875\n",
      "patience is reset, best model is updated\n",
      "epoch: 828 train loss:  26423.539306640625, valid loss:  26763.66015625\n",
      "patience is reset, best model is updated\n",
      "epoch: 829 train loss:  26403.08984375, valid loss:  26857.3203125\n",
      "epoch: 830 train loss:  26431.021240234375, valid loss:  26814.31640625\n",
      "epoch: 831 train loss:  26415.491943359375, valid loss:  26880.439453125\n",
      "epoch: 832 train loss:  26453.93994140625, valid loss:  26792.943359375\n",
      "epoch: 833 train loss:  26410.75390625, valid loss:  26800.4765625\n",
      "epoch: 834 train loss:  26445.412841796875, valid loss:  26823.810546875\n",
      "epoch: 835 train loss:  26395.8359375, valid loss:  26806.767578125\n",
      "epoch: 836 train loss:  26427.1171875, valid loss:  26846.166015625\n",
      "epoch: 837 train loss:  26390.033447265625, valid loss:  26796.998046875\n",
      "epoch: 838 train loss:  26417.3056640625, valid loss:  26801.599609375\n",
      "epoch: 839 train loss:  26386.451416015625, valid loss:  26797.3046875\n",
      "epoch: 840 train loss:  26409.800537109375, valid loss:  26805.26171875\n",
      "epoch: 841 train loss:  26386.60986328125, valid loss:  26832.99609375\n",
      "epoch: 842 train loss:  26405.19580078125, valid loss:  26828.92578125\n",
      "epoch: 843 train loss:  26394.709228515625, valid loss:  26874.9609375\n",
      "epoch: 844 train loss:  26414.470703125, valid loss:  26846.43359375\n",
      "epoch: 845 train loss:  26400.9013671875, valid loss:  26877.451171875\n",
      "epoch: 846 train loss:  26431.152099609375, valid loss:  26856.68359375\n",
      "epoch: 847 train loss:  26401.77392578125, valid loss:  26834.865234375\n",
      "epoch: 848 train loss:  26435.9658203125, valid loss:  26910.37890625\n",
      "epoch: 849 train loss:  26397.626953125, valid loss:  26859.791015625\n",
      "epoch: 850 train loss:  26433.585693359375, valid loss:  26880.66796875\n",
      "epoch: 851 train loss:  26400.6279296875, valid loss:  26832.400390625\n",
      "epoch: 852 train loss:  26420.40185546875, valid loss:  26836.251953125\n",
      "epoch: 853 train loss:  26400.29541015625, valid loss:  26948.263671875\n",
      "epoch: 854 train loss:  26423.806884765625, valid loss:  26921.861328125\n",
      "epoch: 855 train loss:  26410.12255859375, valid loss:  27023.564453125\n",
      "epoch: 856 train loss:  26449.511474609375, valid loss:  26998.50390625\n",
      "epoch: 857 train loss:  26425.7685546875, valid loss:  27057.669921875\n",
      "epoch: 858 train loss:  26470.24609375, valid loss:  26973.1796875\n",
      "epoch: 859 train loss:  26451.464599609375, valid loss:  26997.458984375\n",
      "epoch: 860 train loss:  26468.609619140625, valid loss:  27063.849609375\n",
      "epoch: 861 train loss:  26462.16552734375, valid loss:  27179.501953125\n",
      "epoch: 862 train loss:  26520.07568359375, valid loss:  27101.650390625\n",
      "epoch: 863 train loss:  26523.30419921875, valid loss:  27158.166015625\n",
      "epoch: 864 train loss:  26585.83984375, valid loss:  26996.36328125\n",
      "epoch: 865 train loss:  26595.929443359375, valid loss:  26983.66015625\n",
      "epoch: 866 train loss:  26658.93359375, valid loss:  26893.09765625\n",
      "epoch: 867 train loss:  26612.8076171875, valid loss:  26826.12109375\n",
      "epoch: 868 train loss:  26670.8955078125, valid loss:  27217.1015625\n",
      "epoch: 869 train loss:  26604.576171875, valid loss:  27202.65625\n",
      "epoch: 870 train loss:  26657.93359375, valid loss:  27318.0078125\n",
      "epoch: 871 train loss:  26568.195068359375, valid loss:  27044.99609375\n",
      "epoch: 872 train loss:  26535.122802734375, valid loss:  26977.02734375\n",
      "epoch: 873 train loss:  26464.797607421875, valid loss:  26851.927734375\n",
      "epoch: 874 train loss:  26417.083984375, valid loss:  26778.583984375\n",
      "epoch: 875 train loss:  26362.066650390625, valid loss:  26799.705078125\n",
      "epoch: 876 train loss:  26342.661376953125, valid loss:  26793.21484375\n",
      "epoch: 877 train loss:  26307.869384765625, valid loss:  26757.076171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 878 train loss:  26310.232421875, valid loss:  26784.125\n",
      "epoch: 879 train loss:  26300.183349609375, valid loss:  26772.05078125\n",
      "epoch: 880 train loss:  26321.655517578125, valid loss:  26785.263671875\n",
      "epoch: 881 train loss:  26313.024658203125, valid loss:  26726.56640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 882 train loss:  26334.3583984375, valid loss:  26751.080078125\n",
      "epoch: 883 train loss:  26319.789306640625, valid loss:  26712.60546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 884 train loss:  26341.454833984375, valid loss:  26781.865234375\n",
      "epoch: 885 train loss:  26321.73486328125, valid loss:  26714.771484375\n",
      "epoch: 886 train loss:  26360.7822265625, valid loss:  26814.439453125\n",
      "epoch: 887 train loss:  26334.563720703125, valid loss:  26741.93359375\n",
      "epoch: 888 train loss:  26375.40966796875, valid loss:  26846.25\n",
      "epoch: 889 train loss:  26346.868408203125, valid loss:  26744.53125\n",
      "epoch: 890 train loss:  26383.55908203125, valid loss:  26879.326171875\n",
      "epoch: 891 train loss:  26332.56298828125, valid loss:  26753.966796875\n",
      "epoch: 892 train loss:  26377.675537109375, valid loss:  26964.43359375\n",
      "epoch: 893 train loss:  26334.2841796875, valid loss:  26784.958984375\n",
      "epoch: 894 train loss:  26368.768798828125, valid loss:  26939.380859375\n",
      "epoch: 895 train loss:  26332.822265625, valid loss:  26770.673828125\n",
      "epoch: 896 train loss:  26342.826171875, valid loss:  26831.91015625\n",
      "epoch: 897 train loss:  26316.933349609375, valid loss:  26686.228515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 898 train loss:  26326.877685546875, valid loss:  26699.2109375\n",
      "epoch: 899 train loss:  26314.728759765625, valid loss:  26677.279296875\n",
      "patience is reset, best model is updated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 900 train loss:  26322.64599609375, valid loss:  26675.390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 901 train loss:  26317.045166015625, valid loss:  26706.98828125\n",
      "epoch: 902 train loss:  26321.007568359375, valid loss:  26676.75390625\n",
      "epoch: 903 train loss:  26306.3935546875, valid loss:  26731.55078125\n",
      "epoch: 904 train loss:  26315.95751953125, valid loss:  26723.3203125\n",
      "epoch: 905 train loss:  26307.2314453125, valid loss:  26737.943359375\n",
      "epoch: 906 train loss:  26323.2529296875, valid loss:  26756.16796875\n",
      "epoch: 907 train loss:  26320.358642578125, valid loss:  26808.455078125\n",
      "epoch: 908 train loss:  26349.454345703125, valid loss:  26836.28125\n",
      "epoch: 909 train loss:  26363.263427734375, valid loss:  26858.328125\n",
      "epoch: 910 train loss:  26415.08544921875, valid loss:  26804.32421875\n",
      "epoch: 911 train loss:  26395.619384765625, valid loss:  26799.775390625\n",
      "epoch: 912 train loss:  26474.024169921875, valid loss:  26921.306640625\n",
      "epoch: 913 train loss:  26464.60546875, valid loss:  26785.541015625\n",
      "epoch: 914 train loss:  26471.584716796875, valid loss:  26843.248046875\n",
      "epoch: 915 train loss:  26484.461181640625, valid loss:  26964.302734375\n",
      "epoch: 916 train loss:  26537.044921875, valid loss:  27024.068359375\n",
      "epoch: 917 train loss:  26533.3095703125, valid loss:  27057.025390625\n",
      "epoch: 918 train loss:  26557.79833984375, valid loss:  27370.353515625\n",
      "epoch: 919 train loss:  26558.703857421875, valid loss:  27160.189453125\n",
      "epoch: 920 train loss:  26555.01416015625, valid loss:  27257.95703125\n",
      "epoch: 921 train loss:  26574.984130859375, valid loss:  27289.580078125\n",
      "epoch: 922 train loss:  26611.047607421875, valid loss:  27112.384765625\n",
      "epoch: 923 train loss:  26597.118896484375, valid loss:  27118.158203125\n",
      "epoch: 924 train loss:  26613.140869140625, valid loss:  26977.9609375\n",
      "epoch: 925 train loss:  26660.197998046875, valid loss:  27081.59375\n",
      "epoch: 926 train loss:  26660.35498046875, valid loss:  27058.845703125\n",
      "epoch: 927 train loss:  26585.548583984375, valid loss:  27097.328125\n",
      "epoch: 928 train loss:  26615.205810546875, valid loss:  27395.921875\n",
      "epoch: 929 train loss:  26569.850830078125, valid loss:  27256.734375\n",
      "epoch: 930 train loss:  26565.693359375, valid loss:  27147.01953125\n",
      "epoch: 931 train loss:  26531.589599609375, valid loss:  26855.21875\n",
      "epoch: 932 train loss:  26447.7998046875, valid loss:  26797.998046875\n",
      "epoch: 933 train loss:  26381.09375, valid loss:  26957.193359375\n",
      "epoch: 934 train loss:  26366.459716796875, valid loss:  26970.6953125\n",
      "epoch: 935 train loss:  26359.636474609375, valid loss:  26970.896484375\n",
      "epoch: 936 train loss:  26388.642822265625, valid loss:  26871.671875\n",
      "epoch: 937 train loss:  26377.576416015625, valid loss:  26716.755859375\n",
      "epoch: 938 train loss:  26388.62841796875, valid loss:  26790.818359375\n",
      "epoch: 939 train loss:  26345.71826171875, valid loss:  26796.212890625\n",
      "epoch: 940 train loss:  26380.702392578125, valid loss:  26843.619140625\n",
      "epoch: 941 train loss:  26334.000732421875, valid loss:  26791.146484375\n",
      "epoch: 942 train loss:  26371.750732421875, valid loss:  26754.6328125\n",
      "epoch: 943 train loss:  26325.180908203125, valid loss:  26644.076171875\n",
      "patience is reset, best model is updated\n",
      "epoch: 944 train loss:  26329.403564453125, valid loss:  26646.533203125\n",
      "epoch: 945 train loss:  26277.1904296875, valid loss:  26680.931640625\n",
      "epoch: 946 train loss:  26271.9765625, valid loss:  26641.578125\n",
      "patience is reset, best model is updated\n",
      "epoch: 947 train loss:  26261.78369140625, valid loss:  26687.666015625\n",
      "epoch: 948 train loss:  26270.500732421875, valid loss:  26569.591796875\n",
      "patience is reset, best model is updated\n",
      "epoch: 949 train loss:  26264.859619140625, valid loss:  26633.240234375\n",
      "epoch: 950 train loss:  26285.855224609375, valid loss:  26588.900390625\n",
      "epoch: 951 train loss:  26256.54443359375, valid loss:  26624.279296875\n",
      "epoch: 952 train loss:  26284.421630859375, valid loss:  26630.794921875\n",
      "epoch: 953 train loss:  26237.804443359375, valid loss:  26566.044921875\n",
      "patience is reset, best model is updated\n",
      "epoch: 954 train loss:  26277.883056640625, valid loss:  26673.421875\n",
      "epoch: 955 train loss:  26233.079345703125, valid loss:  26639.41015625\n",
      "epoch: 956 train loss:  26275.03125, valid loss:  26782.7421875\n",
      "epoch: 957 train loss:  26238.169677734375, valid loss:  26658.337890625\n",
      "epoch: 958 train loss:  26254.703857421875, valid loss:  26702.880859375\n",
      "epoch: 959 train loss:  26219.646484375, valid loss:  26600.580078125\n",
      "epoch: 960 train loss:  26227.963134765625, valid loss:  26649.623046875\n",
      "epoch: 961 train loss:  26204.072021484375, valid loss:  26624.515625\n",
      "epoch: 962 train loss:  26209.876953125, valid loss:  26627.25\n",
      "epoch: 963 train loss:  26191.4638671875, valid loss:  26592.94140625\n",
      "epoch: 964 train loss:  26198.255615234375, valid loss:  26564.25390625\n",
      "patience is reset, best model is updated\n",
      "epoch: 965 train loss:  26178.493408203125, valid loss:  26560.705078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 966 train loss:  26191.270263671875, valid loss:  26574.6328125\n",
      "epoch: 967 train loss:  26178.507568359375, valid loss:  26606.357421875\n",
      "epoch: 968 train loss:  26202.397705078125, valid loss:  26622.736328125\n",
      "epoch: 969 train loss:  26187.89306640625, valid loss:  26643.90625\n",
      "epoch: 970 train loss:  26215.84326171875, valid loss:  26615.7265625\n",
      "epoch: 971 train loss:  26196.0205078125, valid loss:  26623.75390625\n",
      "epoch: 972 train loss:  26222.233154296875, valid loss:  26579.07421875\n",
      "epoch: 973 train loss:  26198.17431640625, valid loss:  26609.55078125\n",
      "epoch: 974 train loss:  26229.56591796875, valid loss:  26572.939453125\n",
      "epoch: 975 train loss:  26210.076171875, valid loss:  26629.34765625\n",
      "epoch: 976 train loss:  26255.4560546875, valid loss:  26607.462890625\n",
      "epoch: 977 train loss:  26217.485107421875, valid loss:  26630.0078125\n",
      "epoch: 978 train loss:  26268.604248046875, valid loss:  26597.15234375\n",
      "epoch: 979 train loss:  26207.3583984375, valid loss:  26583.705078125\n",
      "epoch: 980 train loss:  26240.898193359375, valid loss:  26607.123046875\n",
      "epoch: 981 train loss:  26182.75341796875, valid loss:  26572.677734375\n",
      "epoch: 982 train loss:  26206.7822265625, valid loss:  26647.251953125\n",
      "epoch: 983 train loss:  26167.947265625, valid loss:  26598.255859375\n",
      "epoch: 984 train loss:  26192.015869140625, valid loss:  26699.74609375\n",
      "epoch: 985 train loss:  26161.88134765625, valid loss:  26602.14453125\n",
      "epoch: 986 train loss:  26193.830322265625, valid loss:  26665.744140625\n",
      "epoch: 987 train loss:  26161.845703125, valid loss:  26566.873046875\n",
      "epoch: 988 train loss:  26184.5205078125, valid loss:  26644.185546875\n",
      "epoch: 989 train loss:  26155.811279296875, valid loss:  26583.10546875\n",
      "epoch: 990 train loss:  26178.543212890625, valid loss:  26673.25\n",
      "epoch: 991 train loss:  26156.291015625, valid loss:  26631.0859375\n",
      "epoch: 992 train loss:  26173.3681640625, valid loss:  26691.228515625\n",
      "epoch: 993 train loss:  26156.294677734375, valid loss:  26624.32421875\n",
      "epoch: 994 train loss:  26175.7734375, valid loss:  26671.1171875\n",
      "epoch: 995 train loss:  26154.504638671875, valid loss:  26578.673828125\n",
      "epoch: 996 train loss:  26181.23876953125, valid loss:  26647.298828125\n",
      "epoch: 997 train loss:  26162.283935546875, valid loss:  26610.85546875\n",
      "epoch: 998 train loss:  26182.718017578125, valid loss:  26678.216796875\n",
      "epoch: 999 train loss:  26173.8896484375, valid loss:  26635.98828125\n",
      "epoch: 1000 train loss:  26209.577880859375, valid loss:  26617.423828125\n",
      "epoch: 1001 train loss:  26191.419921875, valid loss:  26548.80078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 1002 train loss:  26214.64990234375, valid loss:  26529.904296875\n",
      "patience is reset, best model is updated\n",
      "epoch: 1003 train loss:  26198.999267578125, valid loss:  26532.724609375\n",
      "epoch: 1004 train loss:  26212.83154296875, valid loss:  26557.634765625\n",
      "epoch: 1005 train loss:  26191.282470703125, valid loss:  26582.162109375\n",
      "epoch: 1006 train loss:  26205.80322265625, valid loss:  26565.732421875\n",
      "epoch: 1007 train loss:  26176.260009765625, valid loss:  26581.939453125\n",
      "epoch: 1008 train loss:  26181.31005859375, valid loss:  26564.85546875\n",
      "epoch: 1009 train loss:  26162.177978515625, valid loss:  26589.78515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1010 train loss:  26168.983642578125, valid loss:  26553.5625\n",
      "epoch: 1011 train loss:  26160.87158203125, valid loss:  26615.439453125\n",
      "epoch: 1012 train loss:  26169.06396484375, valid loss:  26601.7109375\n",
      "epoch: 1013 train loss:  26170.94482421875, valid loss:  26695.736328125\n",
      "epoch: 1014 train loss:  26182.841064453125, valid loss:  26614.60546875\n",
      "epoch: 1015 train loss:  26196.842041015625, valid loss:  26667.779296875\n",
      "epoch: 1016 train loss:  26208.640625, valid loss:  26652.6640625\n",
      "epoch: 1017 train loss:  26231.7685546875, valid loss:  26781.0\n",
      "epoch: 1018 train loss:  26237.969482421875, valid loss:  26721.1796875\n",
      "epoch: 1019 train loss:  26245.0947265625, valid loss:  26809.373046875\n",
      "epoch: 1020 train loss:  26261.9365234375, valid loss:  26680.20703125\n",
      "epoch: 1021 train loss:  26255.222900390625, valid loss:  26794.3125\n",
      "epoch: 1022 train loss:  26307.08935546875, valid loss:  26648.818359375\n",
      "epoch: 1023 train loss:  26283.6494140625, valid loss:  26623.685546875\n",
      "epoch: 1024 train loss:  26316.67041015625, valid loss:  26751.798828125\n",
      "epoch: 1025 train loss:  26293.616943359375, valid loss:  26770.921875\n",
      "epoch: 1026 train loss:  26371.748779296875, valid loss:  26870.06640625\n",
      "epoch: 1027 train loss:  26319.875, valid loss:  26898.697265625\n",
      "epoch: 1028 train loss:  26344.945556640625, valid loss:  27104.30859375\n",
      "epoch: 1029 train loss:  26348.359130859375, valid loss:  26879.5546875\n",
      "epoch: 1030 train loss:  26362.10595703125, valid loss:  26913.6015625\n",
      "epoch: 1031 train loss:  26333.550048828125, valid loss:  26692.556640625\n",
      "epoch: 1032 train loss:  26306.255859375, valid loss:  26691.6484375\n",
      "epoch: 1033 train loss:  26257.12109375, valid loss:  26832.58203125\n",
      "epoch: 1034 train loss:  26233.989013671875, valid loss:  26676.443359375\n",
      "epoch: 1035 train loss:  26208.8720703125, valid loss:  26700.759765625\n",
      "epoch: 1036 train loss:  26199.91650390625, valid loss:  26589.970703125\n",
      "epoch: 1037 train loss:  26203.081298828125, valid loss:  26618.3671875\n",
      "epoch: 1038 train loss:  26211.135009765625, valid loss:  26670.0078125\n",
      "epoch: 1039 train loss:  26192.674072265625, valid loss:  26646.111328125\n",
      "epoch: 1040 train loss:  26214.412841796875, valid loss:  26734.037109375\n",
      "epoch: 1041 train loss:  26194.1591796875, valid loss:  26602.46875\n",
      "epoch: 1042 train loss:  26230.946533203125, valid loss:  26560.208984375\n",
      "epoch: 1043 train loss:  26193.294921875, valid loss:  26514.580078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 1044 train loss:  26194.961669921875, valid loss:  26532.91796875\n",
      "epoch: 1045 train loss:  26178.532958984375, valid loss:  26582.849609375\n",
      "epoch: 1046 train loss:  26208.204345703125, valid loss:  26591.02734375\n",
      "epoch: 1047 train loss:  26199.187744140625, valid loss:  26565.189453125\n",
      "epoch: 1048 train loss:  26226.5869140625, valid loss:  26640.810546875\n",
      "epoch: 1049 train loss:  26184.26025390625, valid loss:  26655.5390625\n",
      "epoch: 1050 train loss:  26228.010498046875, valid loss:  26789.40625\n",
      "epoch: 1051 train loss:  26205.1650390625, valid loss:  26783.75390625\n",
      "epoch: 1052 train loss:  26270.645263671875, valid loss:  26959.63671875\n",
      "epoch: 1053 train loss:  26261.22119140625, valid loss:  26933.611328125\n",
      "epoch: 1054 train loss:  26320.202880859375, valid loss:  27019.65625\n",
      "epoch: 1055 train loss:  26316.042236328125, valid loss:  27003.9765625\n",
      "epoch: 1056 train loss:  26352.693359375, valid loss:  26888.501953125\n",
      "epoch: 1057 train loss:  26356.115478515625, valid loss:  26811.484375\n",
      "epoch: 1058 train loss:  26350.64111328125, valid loss:  26756.478515625\n",
      "epoch: 1059 train loss:  26344.172119140625, valid loss:  27018.908203125\n",
      "epoch: 1060 train loss:  26348.62255859375, valid loss:  26874.248046875\n",
      "epoch: 1061 train loss:  26372.1435546875, valid loss:  26767.951171875\n",
      "epoch: 1062 train loss:  26405.337646484375, valid loss:  26979.21875\n",
      "epoch: 1063 train loss:  26353.63818359375, valid loss:  26778.767578125\n",
      "epoch: 1064 train loss:  26344.943359375, valid loss:  26782.0859375\n",
      "epoch: 1065 train loss:  26280.68115234375, valid loss:  26808.775390625\n",
      "epoch: 1066 train loss:  26278.635009765625, valid loss:  26837.63671875\n",
      "epoch: 1067 train loss:  26218.740966796875, valid loss:  26656.984375\n",
      "epoch: 1068 train loss:  26214.244384765625, valid loss:  26644.685546875\n",
      "epoch: 1069 train loss:  26160.68017578125, valid loss:  26580.91796875\n",
      "epoch: 1070 train loss:  26155.612060546875, valid loss:  26485.658203125\n",
      "patience is reset, best model is updated\n",
      "epoch: 1071 train loss:  26117.934814453125, valid loss:  26499.771484375\n",
      "epoch: 1072 train loss:  26118.0869140625, valid loss:  26498.662109375\n",
      "epoch: 1073 train loss:  26092.6953125, valid loss:  26510.958984375\n",
      "epoch: 1074 train loss:  26100.877685546875, valid loss:  26500.962890625\n",
      "epoch: 1075 train loss:  26073.337890625, valid loss:  26531.576171875\n",
      "epoch: 1076 train loss:  26087.300537109375, valid loss:  26516.31640625\n",
      "epoch: 1077 train loss:  26065.2802734375, valid loss:  26541.642578125\n",
      "epoch: 1078 train loss:  26081.927490234375, valid loss:  26534.9296875\n",
      "epoch: 1079 train loss:  26061.93359375, valid loss:  26532.064453125\n",
      "epoch: 1080 train loss:  26084.341796875, valid loss:  26515.533203125\n",
      "epoch: 1081 train loss:  26064.392578125, valid loss:  26509.966796875\n",
      "epoch: 1082 train loss:  26083.27001953125, valid loss:  26494.578125\n",
      "epoch: 1083 train loss:  26061.2978515625, valid loss:  26480.6328125\n",
      "patience is reset, best model is updated\n",
      "epoch: 1084 train loss:  26084.637939453125, valid loss:  26500.388671875\n",
      "epoch: 1085 train loss:  26061.2041015625, valid loss:  26462.24609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 1086 train loss:  26088.411376953125, valid loss:  26506.205078125\n",
      "epoch: 1087 train loss:  26065.892578125, valid loss:  26458.224609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 1088 train loss:  26095.37060546875, valid loss:  26529.62109375\n",
      "epoch: 1089 train loss:  26073.718017578125, valid loss:  26485.13671875\n",
      "epoch: 1090 train loss:  26100.6689453125, valid loss:  26586.771484375\n",
      "epoch: 1091 train loss:  26086.7666015625, valid loss:  26526.81640625\n",
      "epoch: 1092 train loss:  26109.337890625, valid loss:  26623.671875\n",
      "epoch: 1093 train loss:  26096.288818359375, valid loss:  26580.31640625\n",
      "epoch: 1094 train loss:  26124.396240234375, valid loss:  26644.169921875\n",
      "epoch: 1095 train loss:  26103.540283203125, valid loss:  26570.916015625\n",
      "epoch: 1096 train loss:  26124.389892578125, valid loss:  26654.896484375\n",
      "epoch: 1097 train loss:  26111.3818359375, valid loss:  26577.353515625\n",
      "epoch: 1098 train loss:  26120.331787109375, valid loss:  26613.01171875\n",
      "epoch: 1099 train loss:  26100.1787109375, valid loss:  26563.501953125\n",
      "epoch: 1100 train loss:  26104.158447265625, valid loss:  26585.9296875\n",
      "epoch: 1101 train loss:  26091.67578125, valid loss:  26567.314453125\n",
      "epoch: 1102 train loss:  26102.1162109375, valid loss:  26553.24609375\n",
      "epoch: 1103 train loss:  26098.07275390625, valid loss:  26602.5\n",
      "epoch: 1104 train loss:  26106.38671875, valid loss:  26559.5546875\n",
      "epoch: 1105 train loss:  26107.950927734375, valid loss:  26680.041015625\n",
      "epoch: 1106 train loss:  26117.430908203125, valid loss:  26589.21484375\n",
      "epoch: 1107 train loss:  26129.684326171875, valid loss:  26694.3515625\n",
      "epoch: 1108 train loss:  26148.87255859375, valid loss:  26533.1171875\n",
      "epoch: 1109 train loss:  26152.134521484375, valid loss:  26584.080078125\n",
      "epoch: 1110 train loss:  26188.62548828125, valid loss:  26462.6953125\n",
      "epoch: 1111 train loss:  26183.8037109375, valid loss:  26503.8828125\n",
      "epoch: 1112 train loss:  26256.224609375, valid loss:  26687.16015625\n",
      "epoch: 1113 train loss:  26213.03271484375, valid loss:  26875.28125\n",
      "epoch: 1114 train loss:  26323.627685546875, valid loss:  27192.203125\n",
      "epoch: 1115 train loss:  26330.197509765625, valid loss:  27106.521484375\n",
      "epoch: 1116 train loss:  26455.50439453125, valid loss:  26952.462890625\n",
      "epoch: 1117 train loss:  26409.788330078125, valid loss:  26877.310546875\n",
      "epoch: 1118 train loss:  26343.681396484375, valid loss:  26764.04296875\n",
      "epoch: 1119 train loss:  26288.673828125, valid loss:  26783.94140625\n",
      "epoch: 1120 train loss:  26247.635986328125, valid loss:  26627.21875\n",
      "epoch: 1121 train loss:  26191.24267578125, valid loss:  26592.67578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1122 train loss:  26180.69970703125, valid loss:  26686.193359375\n",
      "epoch: 1123 train loss:  26158.1240234375, valid loss:  26535.365234375\n",
      "epoch: 1124 train loss:  26313.80126953125, valid loss:  26617.05859375\n",
      "epoch: 1125 train loss:  26682.757080078125, valid loss:  26705.390625\n",
      "epoch: 1126 train loss:  27675.9931640625, valid loss:  26905.26953125\n",
      "epoch: 1127 train loss:  26297.75390625, valid loss:  26572.1328125\n",
      "epoch: 1128 train loss:  26259.461669921875, valid loss:  26535.6875\n",
      "epoch: 1129 train loss:  26316.500732421875, valid loss:  26566.89453125\n",
      "epoch: 1130 train loss:  26165.859619140625, valid loss:  26486.294921875\n",
      "epoch: 1131 train loss:  26114.199951171875, valid loss:  26488.921875\n",
      "epoch: 1132 train loss:  26066.216064453125, valid loss:  26435.416015625\n",
      "patience is reset, best model is updated\n",
      "epoch: 1133 train loss:  26048.9658203125, valid loss:  26446.80078125\n",
      "epoch: 1134 train loss:  26026.4970703125, valid loss:  26460.251953125\n",
      "epoch: 1135 train loss:  26005.3603515625, valid loss:  26427.78515625\n",
      "patience is reset, best model is updated\n",
      "epoch: 1136 train loss:  25987.927978515625, valid loss:  26413.08984375\n",
      "patience is reset, best model is updated\n",
      "epoch: 1137 train loss:  25972.888671875, valid loss:  26387.560546875\n",
      "patience is reset, best model is updated\n",
      "epoch: 1138 train loss:  25962.209228515625, valid loss:  26380.205078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 1139 train loss:  25956.513427734375, valid loss:  26368.849609375\n",
      "patience is reset, best model is updated\n",
      "epoch: 1140 train loss:  25950.5224609375, valid loss:  26361.5078125\n",
      "patience is reset, best model is updated\n",
      "epoch: 1141 train loss:  25946.321533203125, valid loss:  26360.384765625\n",
      "patience is reset, best model is updated\n",
      "epoch: 1142 train loss:  25942.545654296875, valid loss:  26364.1875\n",
      "epoch: 1143 train loss:  25940.8779296875, valid loss:  26372.99609375\n",
      "epoch: 1144 train loss:  25938.417724609375, valid loss:  26380.58984375\n",
      "epoch: 1145 train loss:  25937.263671875, valid loss:  26388.1953125\n",
      "epoch: 1146 train loss:  25935.606689453125, valid loss:  26389.205078125\n",
      "epoch: 1147 train loss:  25935.144287109375, valid loss:  26387.154296875\n",
      "epoch: 1148 train loss:  25933.744140625, valid loss:  26387.09375\n",
      "epoch: 1149 train loss:  25933.562744140625, valid loss:  26387.267578125\n",
      "epoch: 1150 train loss:  25932.988037109375, valid loss:  26388.611328125\n",
      "epoch: 1151 train loss:  25933.0673828125, valid loss:  26385.953125\n",
      "epoch: 1152 train loss:  25932.040771484375, valid loss:  26378.8125\n",
      "epoch: 1153 train loss:  25931.05126953125, valid loss:  26373.373046875\n",
      "epoch: 1154 train loss:  25927.690185546875, valid loss:  26367.931640625\n",
      "epoch: 1155 train loss:  25929.154296875, valid loss:  26361.43359375\n",
      "epoch: 1156 train loss:  25928.832275390625, valid loss:  26362.78515625\n",
      "epoch: 1157 train loss:  25932.679443359375, valid loss:  26358.931640625\n",
      "patience is reset, best model is updated\n",
      "epoch: 1158 train loss:  25939.28076171875, valid loss:  26371.333984375\n",
      "epoch: 1159 train loss:  25954.891845703125, valid loss:  26377.587890625\n",
      "epoch: 1160 train loss:  25964.288330078125, valid loss:  26394.40234375\n",
      "epoch: 1161 train loss:  26000.382080078125, valid loss:  26408.171875\n",
      "epoch: 1162 train loss:  26013.061279296875, valid loss:  26427.59765625\n",
      "epoch: 1163 train loss:  26084.074462890625, valid loss:  26437.908203125\n",
      "epoch: 1164 train loss:  26066.554931640625, valid loss:  26465.099609375\n",
      "epoch: 1165 train loss:  26144.827392578125, valid loss:  26473.55078125\n",
      "epoch: 1166 train loss:  26078.63134765625, valid loss:  26507.14453125\n",
      "epoch: 1167 train loss:  26127.863037109375, valid loss:  26493.53125\n",
      "epoch: 1168 train loss:  26060.815673828125, valid loss:  26504.87109375\n",
      "epoch: 1169 train loss:  26088.365478515625, valid loss:  26485.603515625\n",
      "epoch: 1170 train loss:  26034.8076171875, valid loss:  26484.404296875\n",
      "epoch: 1171 train loss:  26049.614013671875, valid loss:  26453.595703125\n",
      "epoch: 1172 train loss:  26010.68310546875, valid loss:  26445.8828125\n",
      "epoch: 1173 train loss:  26021.41650390625, valid loss:  26423.986328125\n",
      "epoch: 1174 train loss:  25992.1796875, valid loss:  26414.87109375\n",
      "epoch: 1175 train loss:  25999.971435546875, valid loss:  26404.984375\n",
      "epoch: 1176 train loss:  25978.436767578125, valid loss:  26404.984375\n",
      "epoch: 1177 train loss:  25992.00732421875, valid loss:  26417.859375\n",
      "epoch: 1178 train loss:  25975.7353515625, valid loss:  26422.94921875\n",
      "epoch: 1179 train loss:  25990.555908203125, valid loss:  26439.986328125\n",
      "epoch: 1180 train loss:  25981.154296875, valid loss:  26428.279296875\n",
      "epoch: 1181 train loss:  25999.0849609375, valid loss:  26437.994140625\n",
      "epoch: 1182 train loss:  25989.248046875, valid loss:  26436.99609375\n",
      "epoch: 1183 train loss:  26013.71875, valid loss:  26475.814453125\n",
      "epoch: 1184 train loss:  26003.750732421875, valid loss:  26475.123046875\n",
      "epoch: 1185 train loss:  26032.332763671875, valid loss:  26495.26953125\n",
      "epoch: 1186 train loss:  26020.457763671875, valid loss:  26441.62890625\n",
      "epoch: 1187 train loss:  26047.877685546875, valid loss:  26448.97265625\n",
      "epoch: 1188 train loss:  26023.435791015625, valid loss:  26417.87890625\n",
      "epoch: 1189 train loss:  26043.9609375, valid loss:  26447.15234375\n",
      "epoch: 1190 train loss:  26026.318115234375, valid loss:  26419.927734375\n",
      "epoch: 1191 train loss:  26056.3671875, valid loss:  26428.259765625\n",
      "epoch: 1192 train loss:  26030.844970703125, valid loss:  26423.494140625\n",
      "epoch: 1193 train loss:  26069.13720703125, valid loss:  26473.314453125\n",
      "epoch: 1194 train loss:  26043.669189453125, valid loss:  26479.126953125\n",
      "epoch: 1195 train loss:  26052.52783203125, valid loss:  26487.5\n",
      "epoch: 1196 train loss:  26025.780517578125, valid loss:  26446.056640625\n",
      "epoch: 1197 train loss:  26033.73193359375, valid loss:  26490.576171875\n",
      "epoch: 1198 train loss:  26011.4921875, valid loss:  26468.912109375\n",
      "epoch: 1199 train loss:  26029.6171875, valid loss:  26461.314453125\n",
      "epoch: 1200 train loss:  26006.513427734375, valid loss:  26398.84375\n",
      "epoch: 1201 train loss:  26020.011962890625, valid loss:  26362.357421875\n",
      "epoch: 1202 train loss:  25992.193359375, valid loss:  26379.7109375\n",
      "epoch: 1203 train loss:  26008.536376953125, valid loss:  26398.1328125\n",
      "epoch: 1204 train loss:  25994.0908203125, valid loss:  26424.423828125\n",
      "epoch: 1205 train loss:  26007.881103515625, valid loss:  26412.47265625\n",
      "epoch: 1206 train loss:  25991.580078125, valid loss:  26450.6953125\n",
      "epoch: 1207 train loss:  26005.28857421875, valid loss:  26455.853515625\n",
      "epoch: 1208 train loss:  25993.567626953125, valid loss:  26467.9453125\n",
      "epoch: 1209 train loss:  26007.101806640625, valid loss:  26443.40625\n",
      "epoch: 1210 train loss:  26001.42041015625, valid loss:  26419.185546875\n",
      "epoch: 1211 train loss:  26012.025634765625, valid loss:  26417.421875\n",
      "epoch: 1212 train loss:  26005.408935546875, valid loss:  26423.58203125\n",
      "epoch: 1213 train loss:  26023.116455078125, valid loss:  26404.265625\n",
      "epoch: 1214 train loss:  26022.683349609375, valid loss:  26431.587890625\n",
      "epoch: 1215 train loss:  26042.2626953125, valid loss:  26445.5703125\n",
      "epoch: 1216 train loss:  26025.022705078125, valid loss:  26527.7734375\n",
      "epoch: 1217 train loss:  26041.43505859375, valid loss:  26573.744140625\n",
      "epoch: 1218 train loss:  26031.259765625, valid loss:  26551.884765625\n",
      "epoch: 1219 train loss:  26043.42529296875, valid loss:  26508.5703125\n",
      "epoch: 1220 train loss:  26016.07080078125, valid loss:  26476.369140625\n",
      "epoch: 1221 train loss:  26025.9951171875, valid loss:  26486.15234375\n",
      "epoch: 1222 train loss:  26014.760009765625, valid loss:  26472.87890625\n",
      "epoch: 1223 train loss:  26024.720947265625, valid loss:  26475.609375\n",
      "epoch: 1224 train loss:  26007.108642578125, valid loss:  26508.7265625\n",
      "epoch: 1225 train loss:  26011.06591796875, valid loss:  26555.8359375\n",
      "epoch: 1226 train loss:  26006.201416015625, valid loss:  26514.15625\n",
      "epoch: 1227 train loss:  26006.89990234375, valid loss:  26501.173828125\n",
      "epoch: 1228 train loss:  25994.900390625, valid loss:  26475.47265625\n",
      "epoch: 1229 train loss:  26005.296630859375, valid loss:  26516.408203125\n",
      "epoch: 1230 train loss:  25998.49853515625, valid loss:  26470.615234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1231 train loss:  26008.46435546875, valid loss:  26549.7421875\n",
      "epoch: 1232 train loss:  26006.91748046875, valid loss:  26582.7578125\n",
      "epoch: 1233 train loss:  26039.29248046875, valid loss:  26578.82421875\n",
      "epoch: 1234 train loss:  26024.70263671875, valid loss:  26522.44140625\n",
      "epoch: 1235 train loss:  26046.518798828125, valid loss:  26590.04296875\n",
      "epoch: 1236 train loss:  26044.264892578125, valid loss:  26595.236328125\n",
      "epoch: 1237 train loss:  26073.53515625, valid loss:  26588.306640625\n",
      "epoch: 1238 train loss:  26051.21435546875, valid loss:  26612.244140625\n",
      "epoch: 1239 train loss:  26079.94091796875, valid loss:  26670.15625\n",
      "epoch: 1240 train loss:  26044.7333984375, valid loss:  26608.71875\n",
      "epoch: 1241 train loss:  26066.36767578125, valid loss:  26706.80859375\n",
      "epoch: 1242 train loss:  26048.1572265625, valid loss:  26654.447265625\n",
      "epoch: 1243 train loss:  26073.523193359375, valid loss:  26685.435546875\n",
      "epoch: 1244 train loss:  26057.285400390625, valid loss:  26639.7265625\n",
      "epoch: 1245 train loss:  26084.591552734375, valid loss:  26685.818359375\n",
      "epoch: 1246 train loss:  26077.642578125, valid loss:  26598.994140625\n",
      "epoch: 1247 train loss:  26117.71240234375, valid loss:  26575.509765625\n",
      "epoch: 1248 train loss:  26109.4306640625, valid loss:  26484.455078125\n",
      "epoch: 1249 train loss:  26155.1630859375, valid loss:  26535.845703125\n",
      "epoch: 1250 train loss:  26154.622802734375, valid loss:  26545.888671875\n",
      "epoch: 1251 train loss:  26217.551025390625, valid loss:  26669.326171875\n",
      "epoch: 1252 train loss:  26223.494140625, valid loss:  26735.640625\n",
      "epoch: 1253 train loss:  26259.627685546875, valid loss:  27106.650390625\n",
      "epoch: 1254 train loss:  26273.860595703125, valid loss:  27364.84375\n",
      "epoch: 1255 train loss:  26431.47412109375, valid loss:  27825.951171875\n",
      "epoch: 1256 train loss:  26524.25146484375, valid loss:  27528.17578125\n",
      "epoch: 1257 train loss:  26650.804443359375, valid loss:  26828.587890625\n",
      "epoch: 1258 train loss:  26481.12744140625, valid loss:  26937.302734375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "auto1 = AutoEncoderV1().cuda()\n",
    "optimizer = torch.optim.Adam(auto1.parameters(),lr=0.001)\n",
    "epochs = 20000\n",
    "\n",
    "best_auto1,best_auto1_loss, auto1_his = loop(auto1,trainLoader,validLoader,optimizer,20000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(auto1, './auto_encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(auto1_his['loss'])\n",
    "plt.plot(auto1_his['valid_loss'])\n",
    "plt.title('AutoEncoder MSE Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오토인코더를 통한 특성 추출\n",
    "오토인코더가 인코딩 한 값을 추출하여 numpy 행렬로 변환 \n",
    "훈련데이터, 검증데이터, 테스트 데이터 각각 변환\n",
    "이후 훈련 데이터와 검증 데이터가 합해져서 최종 분류기에 교차 검증에 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = []\n",
    "best_auto1.eval()\n",
    "for data in trainLoader:\n",
    "    output = best_auto1.encode(data.float().cuda()).cpu().detach().numpy()\n",
    "    encoded_train.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2496)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_valid = []\n",
    "for data in validLoader:\n",
    "    output = best_auto1.encode(data.float().cuda()).cpu().detach().numpy()\n",
    "    encoded_valid.append(output)\n",
    "encoded_valid[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2496)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test = []\n",
    "for data in testLoader:\n",
    "    output = best_auto1.encode(data.float().cuda()).cpu().detach().numpy()\n",
    "    encoded_test.append(output)\n",
    "encoded_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1777.37646484,    0.        , 2412.89404297, ...,  690.484375  ,\n",
       "        2050.79589844, 2573.90820312]),\n",
       " array([1777.37646484,    0.        , 2412.89404297, ...,  690.484375  ,\n",
       "        2050.79589844, 2573.90820312]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_encoedMat = np.zeros(shape=(180,2496))\n",
    "encoded_train_val = encoded_train + encoded_valid\n",
    "label_train_val = trainLabels + validLabels\n",
    "i = 0\n",
    "for mats in encoded_train_val:\n",
    "    for mat in mats:\n",
    "        train_val_encoedMat[i] = mat\n",
    "        i += 1\n",
    "train_val_encoedMat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2539.00976562, 4281.24365234, 3472.55249023, ...,  410.47781372,\n",
       "         171.62510681,  842.44482422]),\n",
       " array([2539.0098, 4281.2437, 3472.5525, ...,  410.4778,  171.6251,\n",
       "         842.4448], dtype=float32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_encoedMat[20],encoded_train_val[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((180, 2496), 180)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_encoedMat.shape,len(label_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Based AdaBoost 분류기 구성\n",
    "DecisionTree 분류기를 사용하여 AdaBoost 분류기를 구성\n",
    "AdaBoost를 사용하여 여러개의 약 분류기를 사용하여 이전의 분류기가 학습하기 어려운 샘플에 대해 이후의 분류기들이 점점 더 맞춰지게 학습 수행\n",
    "약 분류기로 트리의 층 수를 제한시킨 DecisionTree 분류기를 수행하여 과적합을 방지\n",
    "트리의 깊이, 분류기의 수, 학습률 등을 3-fold 교차 검증을 수행하여 최적의 모델을 생성함 \n",
    "최적의 모델로 깊이가 1인 Decision Tree, 학습률 1.0, 분류기 갯수 150개가 선택됨. \n",
    "테스트 데이터에 대해서 85의 정확도를 보여줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=8)]: Done 360 out of 360 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1.0, 'n_estimators': 150}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8333    1.0000    0.9091        15\n",
      "           1     1.0000    0.4000    0.5714         5\n",
      "\n",
      "    accuracy                         0.8500        20\n",
      "   macro avg     0.9167    0.7000    0.7403        20\n",
      "weighted avg     0.8750    0.8500    0.8247        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'base_estimator' : [DecisionTreeClassifier(max_depth=1),\n",
    "                     DecisionTreeClassifier(max_depth=2),\n",
    "                     DecisionTreeClassifier(max_depth=3)],\n",
    "    'n_estimators':[50,75,100,125,150,175,200,250],\n",
    "    'learning_rate':[2.0,1.5,1.0,0.75,0.5]\n",
    "}\n",
    "\n",
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "grid = GridSearchCV(ada_clf,params,n_jobs=8,verbose=2,cv=3)\n",
    "grid.fit(train_val_encoedMat,label_train_val)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "out = grid.best_estimator_.predict(encoded_test[0])\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testLabels,out,digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오토인코더를 사용하지 않았을 경우\n",
    "Standard Scaler를 통해서 p signal을 전처리 해준 후에 Decision Tree Based AdaBoost 분류기에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "scaled_train_val_encodedMat = scale.fit_transform(train_val_encoedMat)\n",
    "scaled_test_encodedMat = scale.transform(encoded_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 12, 5000), (20, 12, 5000), (20, 12, 5000))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataToMat(datas):\n",
    "    mat = np.zeros(shape=(len(datas),12,5000))\n",
    "    for i in range(len(datas)):\n",
    "        mat[i] = datas[i].p_signal.T\n",
    "    return mat\n",
    "trainMat = dataToMat(trainDatas)\n",
    "validMat = dataToMat(validDatas)\n",
    "testMat = dataToMat(testDatas)\n",
    "trainMat.shape,validMat.shape,testMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "trainValMat = np.concatenate((trainMat,validMat)).reshape(180,-1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(trainValMat)\n",
    "\n",
    "reshapeTrainValMat = trainValMat.reshape(180,-1)\n",
    "reshapeTestMat = validMat.reshape(20,-1)\n",
    "trainValMat = scaler.transform(reshapeTrainValMat)\n",
    "testMat = scaler.transform(reshapeTestMat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=6)]: Done 150 tasks      | elapsed: 32.5min\n",
      "[Parallel(n_jobs=6)]: Done 360 out of 360 | elapsed: 126.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 0.75, 'n_estimators': 250}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.6667    0.6667        15\n",
      "           1     0.0000    0.0000    0.0000         5\n",
      "\n",
      "    accuracy                         0.5000        20\n",
      "   macro avg     0.3333    0.3333    0.3333        20\n",
      "weighted avg     0.5000    0.5000    0.5000        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'base_estimator' : [DecisionTreeClassifier(max_depth=1),\n",
    "                     DecisionTreeClassifier(max_depth=2),\n",
    "                     DecisionTreeClassifier(max_depth=3)],\n",
    "    'n_estimators':[50,75,100,125,150,175,200,250],\n",
    "    'learning_rate':[2.0,1.5,1.0,0.75,0.5]\n",
    "}\n",
    "\n",
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "grid = GridSearchCV(ada_clf,params,n_jobs=6,verbose=2,cv=3)\n",
    "grid.fit(trainValMat,label_train_val)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "out = grid.best_estimator_.predict(testMat.reshape(20,-1))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testLabels,out,digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Scaler를 적용하지 않은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160, 12, 5000), (20, 12, 5000), (20, 12, 5000))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataToMat(datas):\n",
    "    mat = np.zeros(shape=(len(datas),12,5000))\n",
    "    for i in range(len(datas)):\n",
    "        mat[i] = datas[i].p_signal.T\n",
    "    return mat\n",
    "trainMat = dataToMat(trainDatas)\n",
    "validMat = dataToMat(validDatas)\n",
    "testMat = dataToMat(testDatas)\n",
    "trainMat.shape,validMat.shape,testMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainValMat = np.concatenate((trainMat,validMat)).reshape(180,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1번 레이블을 전혀 분류하지 못함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=8)]: Done 360 out of 360 | elapsed: 98.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator': DecisionTreeClassifier(max_depth=2), 'learning_rate': 1.0, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7368    0.9333    0.8235        15\n",
      "           1     0.0000    0.0000    0.0000         5\n",
      "\n",
      "    accuracy                         0.7000        20\n",
      "   macro avg     0.3684    0.4667    0.4118        20\n",
      "weighted avg     0.5526    0.7000    0.6176        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'base_estimator' : [DecisionTreeClassifier(max_depth=1),\n",
    "                     DecisionTreeClassifier(max_depth=2),\n",
    "                     DecisionTreeClassifier(max_depth=3)],\n",
    "    'n_estimators':[50,75,100,125,150,175,200,250],\n",
    "    'learning_rate':[2.0,1.5,1.0,0.75,0.5]\n",
    "}\n",
    "\n",
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "grid = GridSearchCV(ada_clf,params,n_jobs=8,verbose=2,cv=3)\n",
    "grid.fit(trainValMat,label_train_val)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "out = grid.best_estimator_.predict(testMat.reshape(20,-1))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(testLabels,out,digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_kict_py36_v2",
   "language": "python",
   "name": "py36torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
